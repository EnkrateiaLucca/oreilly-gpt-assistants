{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fb81d93",
   "metadata": {},
   "source": [
    "# Building an Analytics Dashboard Assistant with OpenAI\n",
    "\n",
    "This tutorial will guide you through creating an intelligent analytics assistant using OpenAI's Assistants API. Our assistant will be capable of:\n",
    "- Analyzing multiple data files using File Search\n",
    "- Generating visualizations and insights using Code Interpreter\n",
    "- Creating interactive dashboards based on user queries\n",
    "\n",
    "## Setup and Dependencies\n",
    "\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc5788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai pandas matplotlib seaborn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4e269e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"var: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51b7dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef20d54",
   "metadata": {},
   "source": [
    "## Initializing the OpenAI Client\n",
    "\n",
    "First, we'll set up our OpenAI client with the appropriate API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3403b101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8334a78",
   "metadata": {},
   "source": [
    "## Creating the Analytics Assistant\n",
    "\n",
    "We'll create an assistant that combines both Code Interpreter and File Search capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9dd720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analytics_assistant():\n",
    "    assistant = client.beta.assistants.create(\n",
    "        name=\"Analytics Dashboard Assistant\",\n",
    "        instructions=\"\"\"You are an expert data analyst and visualization specialist. \n",
    "        Your role is to:\n",
    "        1. Analyze data files provided by users\n",
    "        2. Generate insightful visualizations\n",
    "        3. Create comprehensive analytics dashboards\n",
    "        4. Explain trends and patterns in the data\n",
    "        Always provide clear explanations of your analysis process.\"\"\",\n",
    "        model=\"gpt-4o\",\n",
    "        tools=[\n",
    "            {\"type\": \"code_interpreter\"},\n",
    "            {\"type\": \"file_search\"}\n",
    "        ]\n",
    "    )\n",
    "    return assistant\n",
    "\n",
    "analytics_assistant = create_analytics_assistant()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c1f58",
   "metadata": {},
   "source": [
    "## Setting Up the Vector Store for File Search\n",
    "\n",
    "The File Search capability requires setting up a vector store for our data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "394bb6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(name=\"Analytics Files\"):\n",
    "    vector_store = client.beta.vector_stores.create(\n",
    "        name=name,\n",
    "    )\n",
    "    return vector_store\n",
    "\n",
    "def add_files_to_vector_store(vector_store_id, file_ids):\n",
    "    batch = client.beta.vector_stores.file_batches.create_and_poll(\n",
    "        vector_store_id=vector_store_id,\n",
    "        file_ids=file_ids\n",
    "    )\n",
    "    return batch\n",
    "\n",
    "# Create vector store\n",
    "vector_store = create_vector_store()\n",
    "\n",
    "# Update assistant with vector store\n",
    "analytics_assistant = client.beta.assistants.update(\n",
    "    assistant_id=analytics_assistant.id,\n",
    "    tool_resources={\n",
    "        \"file_search\": {\n",
    "            \"vector_store_ids\": [vector_store.id]\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694a074f",
   "metadata": {},
   "source": [
    "## File Upload Helper Functions\n",
    "\n",
    "Let's create helper functions to handle file uploads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0557f5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(file_path):\n",
    "    \"\"\"Upload a file for the assistant to use\"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        response = client.files.create(\n",
    "            file=file,\n",
    "            purpose='assistants'\n",
    "        )\n",
    "    return response\n",
    "\n",
    "def attach_files_to_assistant(assistant_id, file_ids):\n",
    "    \"\"\"Attach files to the assistant for code interpreter\"\"\"\n",
    "    assistant = client.beta.assistants.update(\n",
    "        assistant_id=assistant_id,\n",
    "        tool_resources={\n",
    "            \"code_interpreter\": {\n",
    "                \"file_ids\": file_ids\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    return assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d6239c",
   "metadata": {},
   "source": [
    "## Creating and Managing Threads\n",
    "\n",
    "Now let's create functions to manage conversation threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9181c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_thread_with_files(files=None):\n",
    "    \"\"\"Create a new thread with optional files\"\"\"\n",
    "    if files:\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I've uploaded some files for analysis.\",\n",
    "            \"attachments\": [\n",
    "                {\n",
    "                    \"file_id\": file_id,\n",
    "                    \"tools\": [{\"type\": \"code_interpreter\"}, {\"type\": \"file_search\"}]\n",
    "                } for file_id in files\n",
    "            ]\n",
    "        }]\n",
    "        thread = client.beta.threads.create(messages=messages)\n",
    "    else:\n",
    "        thread = client.beta.threads.create()\n",
    "    return thread\n",
    "\n",
    "def add_message_to_thread(thread_id, content, files=None):\n",
    "    \"\"\"Add a message to an existing thread\"\"\"\n",
    "    if files:\n",
    "        message = client.beta.threads.messages.create(\n",
    "            thread_id=thread_id,\n",
    "            role=\"user\",\n",
    "            content=content,\n",
    "            attachments=[\n",
    "                {\n",
    "                    \"file_id\": file_id,\n",
    "                    \"tools\": [{\"type\": \"code_interpreter\"}, {\"type\": \"file_search\"}]\n",
    "                } for file_id in files\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        message = client.beta.threads.messages.create(\n",
    "            thread_id=thread_id,\n",
    "            role=\"user\",\n",
    "            content=content\n",
    "        )\n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180595c1",
   "metadata": {},
   "source": [
    "## Running the Assistant and Handling Responses\n",
    "\n",
    "Here's how we'll handle running the assistant and processing its responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f54276df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_assistant(thread_id, assistant_id):\n",
    "    \"\"\"Create and manage a run of the assistant\"\"\"\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread_id,\n",
    "        assistant_id=assistant_id\n",
    "    )\n",
    "    \n",
    "    while True:\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread_id,\n",
    "            run_id=run.id\n",
    "        )\n",
    "        \n",
    "        if run.status == 'completed':\n",
    "            break\n",
    "        elif run.status == 'failed':\n",
    "            raise Exception(f\"Run failed: {run.last_error}\")\n",
    "        elif run.status == 'requires_action':\n",
    "            # Handle any required actions (function calls, etc.)\n",
    "            pass\n",
    "        \n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Get messages after run completes\n",
    "    messages = client.beta.threads.messages.list(thread_id=thread_id)\n",
    "    return messages\n",
    "\n",
    "def display_assistant_response(messages):\n",
    "    \"\"\"Display the assistant's response including any generated visualizations\"\"\"\n",
    "    for message in messages:\n",
    "        if message.role == \"assistant\":\n",
    "            for content in message.content:\n",
    "                if content.type == 'text':\n",
    "                    print(content.text.value)\n",
    "                elif content.type == 'image_file':\n",
    "                    # Handle image display\n",
    "                    file_id = content.image_file.file_id\n",
    "                    image_data = client.files.content(file_id)\n",
    "                    # Display image using IPython\n",
    "                    display(HTML(f'<img src=\"data:image/png;base64,{image_data}\" />'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87804e12",
   "metadata": {},
   "source": [
    "## Example Usage: Creating an Analytics Dashboard\n",
    "\n",
    "Let's put it all together with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d03ce93",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Files with extensions [.csv] are not supported for retrieval. See https://platform.openai.com/docs/assistants/tools/file-search/supported-files', 'type': 'invalid_request_error', 'param': None, 'code': 'unsupported_file'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m sales_data \u001b[38;5;241m=\u001b[39m upload_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msales_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# # Add files to vector store\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# add_files_to_vector_store(vector_store.id, [sales_data.id])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create a thread with the files\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m thread \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_thread_with_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msales_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Ask for analysis and dashboard creation\u001b[39;00m\n\u001b[1;32m     11\u001b[0m analysis_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124mPlease analyze the sales and customer data to create a dashboard that shows:\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m1. Monthly sales trends\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124mGenerate visualizations and provide insights for each metric.\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[13], line 14\u001b[0m, in \u001b[0;36mcreate_thread_with_files\u001b[0;34m(files)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m files:\n\u001b[1;32m      4\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [{\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve uploaded some files for analysis.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m         ]\n\u001b[1;32m     13\u001b[0m     }]\n\u001b[0;32m---> 14\u001b[0m     thread \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     thread \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mthreads\u001b[38;5;241m.\u001b[39mcreate()\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/openai/resources/beta/threads/threads.py:130\u001b[0m, in \u001b[0;36mThreads.create\u001b[0;34m(self, messages, metadata, tool_resources, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03mCreate a thread.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m extra_headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI-Beta\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistants=v2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/threads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_resources\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_resources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthread_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mThreadCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mThread\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1282\u001b[0m     )\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/openai/_base_client.py:1064\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1067\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1068\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1073\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'Files with extensions [.csv] are not supported for retrieval. See https://platform.openai.com/docs/assistants/tools/file-search/supported-files', 'type': 'invalid_request_error', 'param': None, 'code': 'unsupported_file'}}"
     ]
    }
   ],
   "source": [
    "# Upload sample data files\n",
    "sales_data = upload_file('sales_data.csv')\n",
    "\n",
    "# # Add files to vector store\n",
    "# add_files_to_vector_store(vector_store.id, [sales_data.id])\n",
    "\n",
    "# Create a thread with the files\n",
    "thread = create_thread_with_files([sales_data.id])\n",
    "\n",
    "# Ask for analysis and dashboard creation\n",
    "analysis_request = \"\"\"\n",
    "Please analyze the sales and customer data to create a dashboard that shows:\n",
    "1. Monthly sales trends\n",
    "2. Customer segmentation by purchase value\n",
    "3. Top-performing products\n",
    "4. Customer retention rates\n",
    "\n",
    "Generate visualizations and provide insights for each metric.\n",
    "\"\"\"\n",
    "\n",
    "add_message_to_thread(thread.id, analysis_request)\n",
    "\n",
    "# Run the assistant\n",
    "messages = run_assistant(thread.id, analytics_assistant.id)\n",
    "\n",
    "# Display the results\n",
    "display_assistant_response(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c44d5e9",
   "metadata": {},
   "source": [
    "## Best Practices and Tips\n",
    "\n",
    "1. **File Management**:\n",
    "   - Keep track of file IDs and clean up unused files\n",
    "   - Use appropriate file formats (CSV, JSON, Excel) for data\n",
    "   - Consider file size limits (512MB per file)\n",
    "\n",
    "2. **Vector Store Organization**:\n",
    "   - Group related files in the same vector store\n",
    "   - Use descriptive names for vector stores\n",
    "   - Monitor vector store expiration policies\n",
    "\n",
    "3. **Error Handling**:\n",
    "   - Implement proper error handling for API calls\n",
    "   - Monitor run status and handle failures gracefully\n",
    "   - Validate file uploads and data formats\n",
    "\n",
    "4. **Performance Optimization**:\n",
    "   - Use appropriate chunk sizes for File Search\n",
    "   - Monitor token usage and context windows\n",
    "   - Implement request rate limiting\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This tutorial demonstrated how to create an intelligent analytics assistant that combines the power of OpenAI's Code Interpreter and File Search capabilities. The assistant can analyze multiple data sources, generate visualizations, and create interactive dashboards based on user queries.\n",
    "\n",
    "You can extend this foundation by:\n",
    "- Adding more sophisticated visualization capabilities\n",
    "- Implementing custom dashboard templates\n",
    "- Adding support for more data formats\n",
    "- Creating specialized analysis functions\n",
    "- Implementing caching for frequently accessed data\n",
    "\n",
    "Remember to handle API keys securely and implement proper error handling in production environments."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
