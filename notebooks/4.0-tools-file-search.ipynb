{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b8b97b",
   "metadata": {},
   "source": [
    "# Tools - File Search\n",
    "\n",
    "This lesson provides a hands-on guide to using the **File Search** feature in OpenAI's Assistants API. Learn how to empower your assistants with the ability to search and retrieve information from your files, making them significantly more knowledgeable and useful.\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "1.  Understand the core concepts of File Search and **Vector Stores**.\n",
    "2.  Upload files and create Vector Stores to manage your assistant's knowledge base.\n",
    "3.  Integrate File Search into your Assistants and Threads.\n",
    "4.  Customize File Search settings for optimal performance.\n",
    "5.  Understand cost management with expiration policies.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**File Search** lets your assistant tap into a wealth of information beyond its base model's training data. By uploading your files, your assistant can leverage proprietary data, user-provided documents, or any other textual information to provide accurate and context-rich responses. OpenAI handles the heavy lifting of processing, embedding, and searching your files, so you can focus on building amazing applications.\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "### 1. Vector Stores: Your Assistant's Knowledge Library\n",
    "\n",
    "*   **Vector Stores** are specialized databases designed for efficient storage and retrieval of information from your files.\n",
    "*   Files added to a Vector Store are automatically processed (parsed, chunked, embedded) to enable both keyword and semantic search.\n",
    "*   Each Vector Store can store up to 10,000 files.\n",
    "*   You can attach Vector Stores to both Assistants and Threads.\n",
    "\n",
    "### 2. File Search in Action\n",
    "\n",
    "1.  **Upload:** Upload your files to OpenAI.\n",
    "2.  **Store:** Create a Vector Store and add your uploaded files.\n",
    "3.  **Attach:** Connect the Vector Store to your Assistant or Thread.\n",
    "4.  **Query:** The Assistant intelligently uses File Search when needed, based on user queries.\n",
    "5.  **Retrieve & Respond:** The Assistant retrieves relevant information from the Vector Store and uses it to craft a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c5fbd",
   "metadata": {},
   "source": [
    "## Example: Building a Research Assistant for Physics Papers\n",
    "\n",
    "Let's build a research assistant that can answer questions based on physics research papers from arXiv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "819e423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"var: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c1f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e5549",
   "metadata": {},
   "source": [
    "### Step 1: Create an Assistant with File Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da10f7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Physics Research Assistant\",\n",
    "    instructions=\"You are a helpful research assistant. Use the provided physics papers to answer questions about the latest research.\",\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e870e",
   "metadata": {},
   "source": [
    "### Step 2: Download Research Papers and Create a Vector Store\n",
    "\n",
    "We'll use two example papers from arXiv. You can download them directly using their URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f31541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Download example physics papers from arXiv\n",
    "paper_urls = [\n",
    "    \"https://arxiv.org/pdf/2503.03718\",  # Example paper 1\n",
    "]\n",
    "file_paths = []\n",
    "\n",
    "for i, url in enumerate(paper_urls):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure the download was successful\n",
    "    file_path = f\"arxiv_paper_{i+1}.pdf\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    file_paths.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dd1f7e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arxiv_paper_1.pdf']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89b7b956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File batch status: completed\n",
      "File counts: FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1)\n"
     ]
    }
   ],
   "source": [
    "# Create a vector store for research papers\n",
    "vector_store = client.beta.vector_stores.create(name=\"Physics Research Papers\")\n",
    "\n",
    "# Prepare the files for upload to OpenAI\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "\n",
    "# Upload files, add them to the vector store, and poll for completion\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    "\n",
    "print(f\"File batch status: {file_batch.status}\")\n",
    "print(f\"File counts: {file_batch.file_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef9ec29",
   "metadata": {},
   "source": [
    "**Important:** Always ensure all files are fully processed (`status: \"completed\"`) before creating runs.\n",
    "\n",
    "### Step 3: Connect the Vector Store to Your Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "416d16aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.update(\n",
    "    assistant_id=assistant.id,\n",
    "    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90f5b5c",
   "metadata": {},
   "source": [
    "### Step 4: Create a Thread and Ask a Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54d71075",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create()\n",
    "\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"How does the utilization of Bayesian optimization techniques in the design of compact LWFA-driven FEL systems contribute to enhanced robustness and reliability against inherent shot-to-shot fluctuations in laser and plasma parameters?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5d3140",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create_and_poll(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    ")\n",
    "\n",
    "run.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "022f75a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: The utilization of Bayesian optimization techniques in the design of compact LWFA-driven FEL systems contributes to enhanced robustness and reliability against inherent shot-to-shot fluctuations in several significant ways:\n",
      "\n",
      "1. **Systematic Quantification and Mitigation of Fluctuations**: Bayesian optimization (BO) is used to effectively quantify and address beam quality fluctuations caused by inherent jitters in laser and plasma parameters such as laser energy variations, focal position displacements, and shock front position instabilities【4:0†arxiv_paper_1.pdf】.\n",
      "\n",
      "2. **Beamline Optimization**: BO techniques optimize the beamline to increase tolerance to these fluctuations, thereby maintaining FEL radiation in a saturated or near-saturated regime. This ensures the system remains robust, even when operating within twice the RMS range of parameter fluctuations【4:0†arxiv_paper_1.pdf】【4:1†arxiv_paper_1.pdf】.\n",
      "\n",
      "3. **Adaptive Mechanism**: The optimization includes parameters such as the strength of quadrupoles and distances between elements. By adapting these parameters, the system achieves a balance that allows for reliable operation despite fluctuations. The Covariance Matrix Adaptation Evolution Strategy (CMA-ES) within the Optuna framework is employed as part of this optimization process to systematically navigate toward optimal solutions【4:5†arxiv_paper_1.pdf】【4:16†arxiv_paper_1.pdf】.\n",
      "\n",
      "4. **Tolerance to Beam Pointing Jitter**: The optimized LWFA-driven FEL system demonstrates a significant tolerance to electron beam pointing jitter, being able to operate with pointing jitters up to 1 mrad while maintaining the desired output energy levels【4:0†arxiv_paper_1.pdf】.\n",
      "\n",
      "5. **Enhanced Gain and Energy Stability**: By optimizing the structures and parameters, the FEL system can maintain high radiation energy levels and robust operation within practical experimental conditions. This stability is critical for achieving high-gain FEL performance【4:2†arxiv_paper_1.pdf】【4:3†arxiv_paper_1.pdf】.\n",
      "\n",
      "Overall, Bayesian optimization plays a critical role in enhancing the robustness and reliability of compact LWFA-driven FEL systems, addressing the key obstacle of shot-to-shot fluctuations by systematically improving the beamline's adaptability and stability under varying experimental conditions【4:0†arxiv_paper_1.pdf】【4:19†arxiv_paper_1.pdf】.\n",
      "\n",
      "user: How does the utilization of Bayesian optimization techniques in the design of compact LWFA-driven FEL systems contribute to enhanced robustness and reliability against inherent shot-to-shot fluctuations in laser and plasma parameters?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "for msg in messages:\n",
    "    print(f\"{msg.role}: {msg.content[0].text.value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "575f2e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"Can you summarize the previous answer in a single paragraph?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85819917",
   "metadata": {},
   "source": [
    "### Step 5: Run the Assistant and Observe File Search in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7931ad73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant > Bayesian optimization significantly enhances the robustness and reliability of compact LWFA-driven FEL systems by methodically addressing inherent shot-to-shot fluctuations in laser and plasma parameters. It optimizes the beamline to increase tolerance to fluctuations such as laser energy, focal position, and shock front instabilities, maintaining operation in a saturated regime. This includes utilizing adaptive mechanisms like tuning quadrupoles and element distances through the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) within the Optuna framework. The system achieves high stability, even with pointing jitters up to 1 mrad, ensuring consistent high-gain FEL performance and energy levels under realistic experimental conditions[0][1][2][3].\n",
      "\n",
      "Citations:\n",
      "[0] arxiv_paper_1.pdf\n",
      "[1] arxiv_paper_1.pdf\n",
      "[2] arxiv_paper_1.pdf\n",
      "[3] arxiv_paper_1.pdf\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler, OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class EventHandler(AssistantEventHandler):\n",
    "    @override\n",
    "    def on_text_created(self, text) -> None:\n",
    "        print(f\"\\nAssistant > \", end=\"\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_tool_call_created(self, tool_call):\n",
    "        print(f\"\\nAssistant > {tool_call.type}\\n\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_message_done(self, message) -> None:\n",
    "        # print a citation to the file searched\n",
    "        message_content = message.content[0].text\n",
    "        annotations = message_content.annotations\n",
    "        citations = []\n",
    "        for index, annotation in enumerate(annotations):\n",
    "            message_content.value = message_content.value.replace(\n",
    "                annotation.text, f\"[{index}]\"\n",
    "            )\n",
    "            if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "                cited_file = client.files.retrieve(file_citation.file_id)\n",
    "                citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "        print(message_content.value)\n",
    "        if citations:\n",
    "            print(\"\\nCitations:\")\n",
    "            print(\"\\n\".join(citations))\n",
    "\n",
    "with client.beta.threads.runs.stream(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    "    instructions=\"Please cite the source paper when answering.\",\n",
    "    event_handler=EventHandler(),\n",
    ") as stream:\n",
    "    stream.until_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485739e8",
   "metadata": {},
   "source": [
    "When you run this code, you'll see the assistant using `file_search` and providing an answer based on the uploaded research papers, citing the source document.\n",
    "\n",
    "## Customizing File Search\n",
    "\n",
    "### Chunking\n",
    "\n",
    "*   **Default:** Chunks of 800 tokens with a 400-token overlap.\n",
    "*   **Customize:** Adjust `max_chunk_size_tokens` (100-4096) and `chunk_overlap_tokens` (0 to `max_chunk_size_tokens / 2`) when adding files to the Vector Store.\n",
    "\n",
    "### Number of Chunks\n",
    "\n",
    "*   **Default:** Up to 20 chunks for `gpt-4*` models, 5 for `gpt-3.5-turbo`.\n",
    "*   **Customize:** Set `file_search.max_num_results` when creating the assistant or run.\n",
    "*   **Note:** Fewer chunks might be returned due to token budget limits or if fewer chunks are available.\n",
    "\n",
    "### Improving Relevance with Ranking\n",
    "\n",
    "*   **Problem:** Low-relevance results can degrade response quality.\n",
    "*   **Solution:** Adjust the ranker settings.\n",
    "    *   `ranker`: Choose the ranking algorithm (`auto` or `default_2024_08_21`).\n",
    "    *   `score_threshold`: Set a minimum relevance score (0.0-1.0) to filter out low-relevance chunks.\n",
    "\n",
    "### Inspecting Search Results\n",
    "Use the `include` parameter in the API to retrieve the specific content used to generate the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4682b25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the run steps to inspect search results\n",
    "run_steps = client.beta.threads.runs.steps.list(\n",
    "    thread_id=thread.id,\n",
    "    run_id=run.id\n",
    ")\n",
    "\n",
    "# Get the first step that used file_search\n",
    "for step in run_steps:\n",
    "    if step.type == \"tool_calls\":\n",
    "        for tool_call in step.step_details.tool_calls:\n",
    "            if tool_call.type == \"file_search\":\n",
    "                # Retrieve the step with detailed search results\n",
    "                run_step = client.beta.threads.runs.steps.retrieve(\n",
    "                    thread_id=thread.id,\n",
    "                    run_id=run.id,\n",
    "                    step_id=step.id,\n",
    "                    include=[\"step_details.tool_calls[*].file_search.results[*].content\"]\n",
    "                )\n",
    "                print(f\"File search step found: {step.id}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efc1b37",
   "metadata": {},
   "source": [
    "## Managing Costs with Expiration Policies\n",
    "\n",
    "*   **Vector Store Costs:** You're billed based on the size of your Vector Stores ($0.10/GB/day after the first free GB).\n",
    "*   **Expiration Policies:** Control costs by setting expiration policies when creating or updating Vector Stores.\n",
    "*   **Default for Thread Vector Stores:** 7 days of inactivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ca26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = client.beta.vector_stores.create(\n",
    "    name=\"Temporary Store\",\n",
    "    expires_after={\"anchor\": \"last_active_at\", \"days\": 3}, # Expires after 3 days of inactivity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690d75bd",
   "metadata": {},
   "source": [
    "## Supported File Types\n",
    "\n",
    "File Search supports a wide range of text-based file formats, including:\n",
    "\n",
    "*   `.pdf`, `.txt`, `.md`, `.docx`, `.html`, `.json`, `.js`, `.py`, and many more (see the documentation for a complete list).\n",
    "*   **Encoding:** `utf-8`, `utf-16`, or `ascii`.\n",
    "*   **Maximum File Size:** 512 MB.\n",
    "*   **Maximum Tokens per File:** 5,000,000.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "File Search empowers your Assistants with the ability to leverage your data, transforming them into powerful tools for research, customer support, analysis, and more. By understanding Vector Stores, customizing settings, and managing costs effectively, you can build sophisticated, knowledge-rich assistants that meet your specific needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-assistants",
   "language": "python",
   "name": "gpt-assistants"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
