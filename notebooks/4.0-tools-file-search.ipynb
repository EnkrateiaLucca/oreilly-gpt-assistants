{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b8b97b",
   "metadata": {},
   "source": [
    "# Tools - File Search\n",
    "\n",
    "This lesson provides a hands-on guide to using the **File Search** feature in OpenAI's Assistants API. Learn how to empower your assistants with the ability to search and retrieve information from your files, making them significantly more knowledgeable and useful.\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "1.  Understand the core concepts of File Search and **Vector Stores**.\n",
    "2.  Upload files and create Vector Stores to manage your assistant's knowledge base.\n",
    "3.  Integrate File Search into your Assistants and Threads.\n",
    "4.  Customize File Search settings for optimal performance.\n",
    "5.  Understand cost management with expiration policies.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**File Search** lets your assistant tap into a wealth of information beyond its base model's training data. By uploading your files, your assistant can leverage proprietary data, user-provided documents, or any other textual information to provide accurate and context-rich responses. OpenAI handles the heavy lifting of processing, embedding, and searching your files, so you can focus on building amazing applications.\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "### 1. Vector Stores: Your Assistant's Knowledge Library\n",
    "\n",
    "*   **Vector Stores** are specialized databases designed for efficient storage and retrieval of information from your files.\n",
    "*   Files added to a Vector Store are automatically processed (parsed, chunked, embedded) to enable both keyword and semantic search.\n",
    "*   Each Vector Store can store up to 10,000 files.\n",
    "*   You can attach Vector Stores to both Assistants and Threads.\n",
    "\n",
    "### 2. File Search in Action\n",
    "\n",
    "1.  **Upload:** Upload your files to OpenAI.\n",
    "2.  **Store:** Create a Vector Store and add your uploaded files.\n",
    "3.  **Attach:** Connect the Vector Store to your Assistant or Thread.\n",
    "4.  **Query:** The Assistant intelligently uses File Search when needed, based on user queries.\n",
    "5.  **Retrieve & Respond:** The Assistant retrieves relevant information from the Vector Store and uses it to craft a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c5fbd",
   "metadata": {},
   "source": [
    "## Example: Building a Research Assistant for Physics Papers\n",
    "\n",
    "Let's build a research assistant that can answer questions based on physics research papers from arXiv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"var: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c1f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e5549",
   "metadata": {},
   "source": [
    "### Step 1: Create an Assistant with File Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10f7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Physics Research Assistant\",\n",
    "    instructions=\"You are a helpful research assistant. Use the provided physics papers to answer questions about the latest research.\",\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e870e",
   "metadata": {},
   "source": [
    "### Step 2: Download Research Papers and Create a Vector Store\n",
    "\n",
    "We'll use two example papers from arXiv. You can download them directly using their URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f31541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Download example physics papers from arXiv\n",
    "paper_urls = [\n",
    "    \"[invalid URL removed]\",  # Example paper 1\n",
    "    \"[invalid URL removed]\",  # Example paper 2\n",
    "]\n",
    "file_paths = []\n",
    "\n",
    "for i, url in enumerate(paper_urls):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure the download was successful\n",
    "    file_path = f\"arxiv_paper_{i+1}.pdf\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    file_paths.append(file_path)\n",
    "\n",
    "# Create a vector store for research papers\n",
    "vector_store = client.beta.vector_stores.create(name=\"Physics Research Papers\")\n",
    "\n",
    "# Prepare the files for upload to OpenAI\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "\n",
    "# Upload files, add them to the vector store, and poll for completion\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    "\n",
    "print(f\"File batch status: {file_batch.status}\")\n",
    "print(f\"File counts: {file_batch.file_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef9ec29",
   "metadata": {},
   "source": [
    "**Important:** Always ensure all files are fully processed (`status: \"completed\"`) before creating runs.\n",
    "\n",
    "### Step 3: Connect the Vector Store to Your Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416d16aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.update(\n",
    "    assistant_id=assistant.id,\n",
    "    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90f5b5c",
   "metadata": {},
   "source": [
    "### Step 4: Create a Thread and Ask a Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d71075",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create()\n",
    "\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"What are the main findings of the paper about causal discovery?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85819917",
   "metadata": {},
   "source": [
    "### Step 5: Run the Assistant and Observe File Search in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7931ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler, OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class EventHandler(AssistantEventHandler):\n",
    "    @override\n",
    "    def on_text_created(self, text) -> None:\n",
    "        print(f\"\\nAssistant > \", end=\"\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_tool_call_created(self, tool_call):\n",
    "        print(f\"\\nAssistant > {tool_call.type}\\n\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_message_done(self, message) -> None:\n",
    "        # print a citation to the file searched\n",
    "        message_content = message.content[0].text\n",
    "        annotations = message_content.annotations\n",
    "        citations = []\n",
    "        for index, annotation in enumerate(annotations):\n",
    "            message_content.value = message_content.value.replace(\n",
    "                annotation.text, f\"[{index}]\"\n",
    "            )\n",
    "            if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "                cited_file = client.files.retrieve(file_citation.file_id)\n",
    "                citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "        print(message_content.value)\n",
    "        if citations:\n",
    "            print(\"\\nCitations:\")\n",
    "            print(\"\\n\".join(citations))\n",
    "\n",
    "with client.beta.threads.runs.stream(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    "    instructions=\"Please cite the source paper when answering.\",\n",
    "    event_handler=EventHandler(),\n",
    ") as stream:\n",
    "    stream.until_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485739e8",
   "metadata": {},
   "source": [
    "When you run this code, you'll see the assistant using `file_search` and providing an answer based on the uploaded research papers, citing the source document.\n",
    "\n",
    "## Customizing File Search\n",
    "\n",
    "### Chunking\n",
    "\n",
    "*   **Default:** Chunks of 800 tokens with a 400-token overlap.\n",
    "*   **Customize:** Adjust `max_chunk_size_tokens` (100-4096) and `chunk_overlap_tokens` (0 to `max_chunk_size_tokens / 2`) when adding files to the Vector Store.\n",
    "\n",
    "### Number of Chunks\n",
    "\n",
    "*   **Default:** Up to 20 chunks for `gpt-4*` models, 5 for `gpt-3.5-turbo`.\n",
    "*   **Customize:** Set `file_search.max_num_results` when creating the assistant or run.\n",
    "*   **Note:** Fewer chunks might be returned due to token budget limits or if fewer chunks are available.\n",
    "\n",
    "### Improving Relevance with Ranking\n",
    "\n",
    "*   **Problem:** Low-relevance results can degrade response quality.\n",
    "*   **Solution:** Adjust the ranker settings.\n",
    "    *   `ranker`: Choose the ranking algorithm (`auto` or `default_2024_08_21`).\n",
    "    *   `score_threshold`: Set a minimum relevance score (0.0-1.0) to filter out low-relevance chunks.\n",
    "\n",
    "### Inspecting Search Results\n",
    "Use the `include` parameter in the API to retrieve the specific content used to generate the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4682b25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_step = client.beta.threads.runs.steps.retrieve(\n",
    "    thread_id=thread.id,\n",
    "    run_id=run.id,\n",
    "    step_id=step.id,\n",
    "    include=[\"step_details.tool_calls[*].file_search.results[*].content\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efc1b37",
   "metadata": {},
   "source": [
    "## Managing Costs with Expiration Policies\n",
    "\n",
    "*   **Vector Store Costs:** You're billed based on the size of your Vector Stores ($0.10/GB/day after the first free GB).\n",
    "*   **Expiration Policies:** Control costs by setting expiration policies when creating or updating Vector Stores.\n",
    "*   **Default for Thread Vector Stores:** 7 days of inactivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ca26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = client.beta.vector_stores.create(\n",
    "    name=\"Temporary Store\",\n",
    "    expires_after={\"anchor\": \"last_active_at\", \"days\": 3}, # Expires after 3 days of inactivity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690d75bd",
   "metadata": {},
   "source": [
    "## Supported File Types\n",
    "\n",
    "File Search supports a wide range of text-based file formats, including:\n",
    "\n",
    "*   `.pdf`, `.txt`, `.md`, `.docx`, `.html`, `.json`, `.js`, `.py`, and many more (see the documentation for a complete list).\n",
    "*   **Encoding:** `utf-8`, `utf-16`, or `ascii`.\n",
    "*   **Maximum File Size:** 512 MB.\n",
    "*   **Maximum Tokens per File:** 5,000,000.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "File Search empowers your Assistants with the ability to leverage your data, transforming them into powerful tools for research, customer support, analysis, and more. By understanding Vector Stores, customizing settings, and managing costs effectively, you can build sophisticated, knowledge-rich assistants that meet your specific needs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
