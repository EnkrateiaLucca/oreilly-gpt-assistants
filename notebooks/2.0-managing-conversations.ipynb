{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6535c83",
   "metadata": {},
   "source": [
    "# Managing Conversations in the OpenAI Responses API\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lesson, we'll explore how to effectively manage conversations in the OpenAI Responses API. The Responses API is the modern replacement for the Assistants API, providing a streamlined, stateful interface for building conversational AI applications.\n",
    "\n",
    "### Key Differences from Assistants API\n",
    "\n",
    "**Assistants API → Responses API Mapping:**\n",
    "- Assistants → Prompts (instructions)\n",
    "- Threads → Conversations (implicit, linked via response IDs)\n",
    "- Runs → Responses\n",
    "- Run-Steps → Items\n",
    "\n",
    "**Main Advantages:**\n",
    "- Server-side conversation state management\n",
    "- No need to manually track threads and message history\n",
    "- Simplified API with less boilerplate code\n",
    "- Built-in tools (web search, file search, code interpreter)\n",
    "- Conversation forking capabilities\n",
    "\n",
    "First, let's set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee730509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f7bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93dfcbc",
   "metadata": {},
   "source": [
    "## Understanding Conversations in the Responses API\n",
    "\n",
    "Unlike the Assistants API which required explicit thread creation and management, the Responses API handles conversations implicitly through response chaining. Each response has a unique ID, and you maintain conversation continuity by referencing the previous response ID.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Responses are stored for 30 days by default\n",
    "- You can disable storage with `store=False`\n",
    "- Conversations are formed by linking responses via `previous_response_id`\n",
    "- All prior input tokens remain billable, even when using `previous_response_id`\n",
    "\n",
    "### Creating a Basic Response\n",
    "\n",
    "Let's start by creating a simple response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927e8aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basic_response(user_input):\n",
    "    \"\"\"Create a basic response without conversation history.\"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        input=user_input\n",
    "    )\n",
    "    print(f\"Response ID: {response.id}\")\n",
    "    print(f\"Output: {response.output[0].content[0].text}\")\n",
    "    return response\n",
    "\n",
    "# Create a new response\n",
    "response = create_basic_response(\"Tell me a joke about programming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfc923d",
   "metadata": {},
   "source": [
    "### Continuing Conversations with previous_response_id\n",
    "\n",
    "To continue a conversation, simply pass the `previous_response_id` parameter. The API automatically retrieves the full conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70016ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_conversation(previous_response_id, user_input):\n",
    "    \"\"\"Continue an existing conversation by referencing the previous response.\"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        input=user_input,\n",
    "        previous_response_id=previous_response_id\n",
    "    )\n",
    "    print(f\"Response ID: {response.id}\")\n",
    "    print(f\"Output: {response.output[0].content[0].text}\")\n",
    "    return response\n",
    "\n",
    "# Continue the conversation\n",
    "response_2 = continue_conversation(response.id, \"Tell me another one!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5957b9",
   "metadata": {},
   "source": [
    "## Managing Conversation State\n",
    "\n",
    "### Using the store Parameter\n",
    "\n",
    "By default, responses are stored on OpenAI's servers (`store=True`). You can disable this for privacy or cost reasons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec113ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ephemeral_response(user_input):\n",
    "    \"\"\"Create a response that won't be stored on OpenAI's servers.\"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        input=user_input,\n",
    "        store=False  # Don't store conversation state\n",
    "    )\n",
    "    print(f\"Response ID: {response.id}\")\n",
    "    print(f\"Output: {response.output[0].content[0].text}\")\n",
    "    return response\n",
    "\n",
    "# Example: Create a response without storing\n",
    "ephemeral_response = create_ephemeral_response(\"What's the weather like?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d4d253",
   "metadata": {},
   "source": [
    "### Retrieving Previous Responses\n",
    "\n",
    "You can retrieve any stored response by its ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5f175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_response(response_id):\n",
    "    \"\"\"Retrieve a previously stored response by its ID.\"\"\"\n",
    "    fetched_response = client.responses.retrieve(response_id=response_id)\n",
    "    print(f\"Retrieved Response ID: {fetched_response.id}\")\n",
    "    print(f\"Output: {fetched_response.output[0].content[0].text}\")\n",
    "    return fetched_response\n",
    "\n",
    "# Example: Retrieve the first response we created\n",
    "retrieved = retrieve_response(response.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2801625d",
   "metadata": {},
   "source": [
    "## Forking Conversations\n",
    "\n",
    "One powerful feature is the ability to fork conversations - branching from any previous response to explore alternative paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba0af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fork_conversation(fork_from_id, user_input):\n",
    "    \"\"\"Fork a conversation from a specific response ID.\"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        input=user_input,\n",
    "        previous_response_id=fork_from_id\n",
    "    )\n",
    "    print(f\"Forked Response ID: {response.id}\")\n",
    "    print(f\"Output: {response.output[0].content[0].text}\")\n",
    "    return response\n",
    "\n",
    "# Fork from the first response with a different question\n",
    "forked_response = fork_conversation(\n",
    "    response.id, \n",
    "    \"Actually, can you explain what makes that joke funny?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d355c88",
   "metadata": {},
   "source": [
    "## Working with Instructions (System Prompts)\n",
    "\n",
    "You can provide instructions to shape the assistant's behavior. Instructions are similar to system prompts in Chat Completions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e1a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_response_with_instructions(instructions, user_input):\n",
    "    \"\"\"Create a response with custom instructions.\"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        instructions=instructions,\n",
    "        input=user_input\n",
    "    )\n",
    "    print(f\"Response: {response.output[0].content[0].text}\")\n",
    "    return response\n",
    "\n",
    "# Example: Create a response with a specific persona\n",
    "pirate_response = create_response_with_instructions(\n",
    "    instructions=\"You are a helpful coding assistant that talks like a pirate.\",\n",
    "    user_input=\"How do I declare a variable in Python?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9838d9",
   "metadata": {},
   "source": [
    "## Working with Different Content Types\n",
    "\n",
    "### Text Messages with Multiple Turns\n",
    "\n",
    "You can structure input with role-based messages for more complex conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec9b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_turn_response():\n",
    "    \"\"\"Create a response with structured message history.\"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"I'm learning about data structures.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Can you explain what a hash table is?\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print(f\"Response: {response.output[0].content[0].text}\")\n",
    "    return response\n",
    "\n",
    "# Example with multiple message turns\n",
    "multi_turn_response = create_multi_turn_response()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5027ac7f",
   "metadata": {},
   "source": [
    "### Messages with Images\n",
    "\n",
    "The Responses API supports multimodal inputs including images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aba79db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image(image_url, question):\n",
    "    \"\"\"Analyze an image by providing a URL.\"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o\",  # Use gpt-4o for vision capabilities\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"input_text\",\n",
    "                        \"text\": question\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"input_image\",\n",
    "                        \"image_url\": image_url\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print(f\"Response: {response.output[0].content[0].text}\")\n",
    "    return response\n",
    "\n",
    "# Example: Analyze an image (uncomment with a real image URL)\n",
    "# image_response = analyze_image(\n",
    "#     \"https://example.com/image.jpg\",\n",
    "#     \"What do you see in this image?\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b215940",
   "metadata": {},
   "source": [
    "### Working with Base64 Images\n",
    "\n",
    "You can also provide images as base64-encoded strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def analyze_local_image(image_path, question):\n",
    "    \"\"\"Analyze a local image file by encoding it as base64.\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    # Determine image format from file extension\n",
    "    image_format = image_path.split('.')[-1].lower()\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o\",\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"input_text\",\n",
    "                        \"text\": question\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"input_image\",\n",
    "                        \"image_url\": f\"data:image/{image_format};base64,{image_data}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print(f\"Response: {response.output[0].content[0].text}\")\n",
    "    return response\n",
    "\n",
    "# Example: Analyze a local image (uncomment with a real image path)\n",
    "# local_image_response = analyze_local_image(\n",
    "#     \"/path/to/image.png\",\n",
    "#     \"Describe this image in detail.\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f3g4h5",
   "metadata": {},
   "source": [
    "## Using Built-in Tools\n",
    "\n",
    "### Web Search Tool\n",
    "\n",
    "The Responses API includes built-in tools like web search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4g5h6i7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web(query):\n",
    "    \"\"\"Use the built-in web search tool.\"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o\",\n",
    "        input=query,\n",
    "        tools=[{\"type\": \"web_search\"}]\n",
    "    )\n",
    "    \n",
    "    # The response may include tool execution results\n",
    "    for item in response.output:\n",
    "        if hasattr(item, 'content'):\n",
    "            for content in item.content:\n",
    "                if hasattr(content, 'text'):\n",
    "                    print(f\"Response: {content.text}\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example: Search for current information\n",
    "# web_response = search_web(\"What are the latest developments in AI in 2025?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g5h6i7j8",
   "metadata": {},
   "source": [
    "### File Search Tool\n",
    "\n",
    "You can enable file search for document retrieval and analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h6i7j8k9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_file_search(query):\n",
    "    \"\"\"Use the built-in file search tool.\"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o\",\n",
    "        input=query,\n",
    "        tools=[{\"type\": \"file_search\"}]\n",
    "    )\n",
    "    print(f\"Response: {response.output[0].content[0].text}\")\n",
    "    return response\n",
    "\n",
    "# Example usage with file search\n",
    "# file_search_response = use_file_search(\"Find information about Python decorators in the documentation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i7j8k9l0",
   "metadata": {},
   "source": [
    "### Code Interpreter Tool\n",
    "\n",
    "Enable the code interpreter for data analysis and code execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j8k9l0m1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_code_interpreter(query):\n",
    "    \"\"\"Use the built-in code interpreter tool.\"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o\",\n",
    "        input=query,\n",
    "        tools=[{\"type\": \"code_interpreter\"}]\n",
    "    )\n",
    "    print(f\"Response: {response.output[0].content[0].text}\")\n",
    "    return response\n",
    "\n",
    "# Example: Request data analysis\n",
    "code_response = use_code_interpreter(\n",
    "    \"Create a Python function to calculate the Fibonacci sequence up to n terms.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k9l0m1n2",
   "metadata": {},
   "source": [
    "## Managing Context Windows and Token Limits\n",
    "\n",
    "You can control token usage with `max_prompt_tokens` and `max_completion_tokens`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l0m1n2o3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_limited_response(user_input):\n",
    "    \"\"\"Create a response with token limits.\"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        input=user_input,\n",
    "        max_prompt_tokens=500,\n",
    "        max_completion_tokens=1000\n",
    "    )\n",
    "    print(f\"Response: {response.output[0].content[0].text}\")\n",
    "    return response\n",
    "\n",
    "# Example with token limits\n",
    "limited_response = create_limited_response(\n",
    "    \"Explain machine learning in simple terms.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m1n2o3p4",
   "metadata": {},
   "source": [
    "### Automatic Truncation\n",
    "\n",
    "For long conversations, use automatic truncation to manage context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n2o3p4q5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_truncated_response(previous_response_id, user_input):\n",
    "    \"\"\"Create a response with automatic truncation for long conversations.\"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        input=user_input,\n",
    "        previous_response_id=previous_response_id,\n",
    "        truncation=\"auto\"  # Automatically truncate to fit context window\n",
    "    )\n",
    "    print(f\"Response: {response.output[0].content[0].text}\")\n",
    "    return response\n",
    "\n",
    "# Example with truncation\n",
    "truncated_response = create_truncated_response(\n",
    "    response.id,\n",
    "    \"Continue our conversation from earlier.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o3p4q5r6",
   "metadata": {},
   "source": [
    "## Complete Conversation Example\n",
    "\n",
    "Let's create a complete multi-turn conversation with state management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p4q5r6s7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_turn_conversation():\n",
    "    \"\"\"Demonstrate a complete multi-turn conversation.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Starting a multi-turn conversation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Turn 1: Initial response\n",
    "    response_1 = client.responses.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        instructions=\"You are a helpful Python programming tutor.\",\n",
    "        input=\"I'm new to Python. What should I learn first?\"\n",
    "    )\n",
    "    print(f\"\\nTurn 1 - Response ID: {response_1.id}\")\n",
    "    print(f\"Assistant: {response_1.output[0].content[0].text}\\n\")\n",
    "    \n",
    "    # Turn 2: Continue conversation\n",
    "    response_2 = client.responses.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        input=\"Can you give me an example of a for loop?\",\n",
    "        previous_response_id=response_1.id\n",
    "    )\n",
    "    print(f\"Turn 2 - Response ID: {response_2.id}\")\n",
    "    print(f\"Assistant: {response_2.output[0].content[0].text}\\n\")\n",
    "    \n",
    "    # Turn 3: Continue further\n",
    "    response_3 = client.responses.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        input=\"What's the difference between a list and a tuple?\",\n",
    "        previous_response_id=response_2.id\n",
    "    )\n",
    "    print(f\"Turn 3 - Response ID: {response_3.id}\")\n",
    "    print(f\"Assistant: {response_3.output[0].content[0].text}\\n\")\n",
    "    \n",
    "    # Turn 4: Fork back to turn 1\n",
    "    response_4 = client.responses.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        input=\"Actually, what about web development with Python instead?\",\n",
    "        previous_response_id=response_1.id  # Fork from first response\n",
    "    )\n",
    "    print(f\"Turn 4 (Forked from Turn 1) - Response ID: {response_4.id}\")\n",
    "    print(f\"Assistant: {response_4.output[0].content[0].text}\\n\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Conversation complete\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return response_1, response_2, response_3, response_4\n",
    "\n",
    "# Run the complete conversation example\n",
    "conv_responses = multi_turn_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q5r6s7t8",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Conversation Management\n",
    "\n",
    "- **Store response IDs**: Keep track of response IDs to continue conversations\n",
    "- **Use forking wisely**: Fork conversations to explore alternative paths without losing context\n",
    "- **Monitor token usage**: Remember that all prior tokens are billable when using `previous_response_id`\n",
    "- **Set storage preferences**: Use `store=False` for sensitive conversations\n",
    "\n",
    "### 2. Performance Optimization\n",
    "\n",
    "- **Use appropriate models**: Use `gpt-4o-mini` for simple tasks, `gpt-4o` for complex reasoning\n",
    "- **Limit context**: Use `max_prompt_tokens` and `truncation=\"auto\"` for long conversations\n",
    "- **Cache responses**: Store frequently accessed responses locally to reduce API calls\n",
    "\n",
    "### 3. Content Best Practices\n",
    "\n",
    "- **Clear instructions**: Provide clear, specific instructions for consistent behavior\n",
    "- **Structured input**: Use role-based messages for complex conversations\n",
    "- **Tool selection**: Choose appropriate tools (web_search, file_search, code_interpreter) based on the task\n",
    "\n",
    "### 4. Error Handling\n",
    "\n",
    "Always implement proper error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r6s7t8u9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_create_response(user_input, previous_response_id=None):\n",
    "    \"\"\"Create a response with proper error handling.\"\"\"\n",
    "    try:\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            input=user_input,\n",
    "            previous_response_id=previous_response_id\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating response: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example with error handling\n",
    "safe_response = safe_create_response(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s7t8u9v0",
   "metadata": {},
   "source": [
    "## Exercise: Build a Conversational Assistant\n",
    "\n",
    "Try this exercise to practice working with conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t8u9v0w1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_conversation():\n",
    "    \"\"\"\n",
    "    Create an interactive conversation loop.\n",
    "    Type 'quit' to exit.\n",
    "    \"\"\"\n",
    "    print(\"Starting interactive conversation. Type 'quit' to exit.\\n\")\n",
    "    \n",
    "    last_response_id = None\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            response = client.responses.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                instructions=\"You are a friendly and helpful assistant.\",\n",
    "                input=user_input,\n",
    "                previous_response_id=last_response_id\n",
    "            )\n",
    "            \n",
    "            last_response_id = response.id\n",
    "            print(f\"\\nAssistant: {response.output[0].content[0].text}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\\n\")\n",
    "\n",
    "# Uncomment to run the interactive conversation\n",
    "# interactive_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u9v0w1x2",
   "metadata": {},
   "source": [
    "## Migration from Assistants API\n",
    "\n",
    "If you're migrating from the Assistants API, here's a quick reference:\n",
    "\n",
    "### Assistants API Pattern\n",
    "```python\n",
    "# Old way (Assistants API)\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"My Assistant\",\n",
    "    instructions=\"You are helpful.\",\n",
    "    model=\"gpt-4\"\n",
    ")\n",
    "\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"Hello\"\n",
    ")\n",
    "\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id\n",
    ")\n",
    "```\n",
    "\n",
    "### Responses API Pattern\n",
    "```python\n",
    "# New way (Responses API)\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    instructions=\"You are helpful.\",\n",
    "    input=\"Hello\"\n",
    ")\n",
    "\n",
    "# Continue conversation\n",
    "response_2 = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"How are you?\",\n",
    "    previous_response_id=response.id\n",
    ")\n",
    "```\n",
    "\n",
    "**Benefits of Migration:**\n",
    "- Simpler API with less boilerplate\n",
    "- Faster response times\n",
    "- Built-in state management\n",
    "- Conversation forking capabilities\n",
    "- Unified interface for tools and multimodal inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v0w1x2y3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The Responses API provides a streamlined, powerful way to build conversational AI applications. Key takeaways:\n",
    "\n",
    "1. **Simplified State Management**: No need to manually manage threads and messages\n",
    "2. **Server-Side Storage**: Conversations are stored automatically for 30 days\n",
    "3. **Flexible Continuation**: Use `previous_response_id` to continue or fork conversations\n",
    "4. **Built-in Tools**: Web search, file search, and code interpreter available out of the box\n",
    "5. **Multimodal Support**: Handle text, images, and files in the same API\n",
    "6. **Cost Management**: Control token usage with limits and truncation\n",
    "\n",
    "The Responses API represents a significant improvement over the Assistants API, offering better performance, simpler code, and more powerful features for building modern AI applications.\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [OpenAI Responses API Documentation](https://platform.openai.com/docs/api-reference/responses)\n",
    "- [OpenAI Cookbook - Responses API Examples](https://cookbook.openai.com/examples/responses_api/responses_example)\n",
    "- [Migration Guide: Assistants API to Responses API](https://apimagic.ai/blog/switching-assistant-responses-api)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
