{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5885d562",
   "metadata": {},
   "source": [
    "# Building AI Applications with OpenAI's Responses API\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to this comprehensive guide on using OpenAI's **Responses API**, the modern replacement for the deprecated Assistants API. In this tutorial, we'll explore how to build powerful AI applications that can search through documents and perform complex data analysis.\n",
    "\n",
    "### What's Changed?\n",
    "\n",
    "The Assistants API is being deprecated (sunset planned for mid-2026) in favor of the new **Responses API**, which offers:\n",
    "\n",
    "- **Simpler architecture**: No more complex thread management - just pass `previous_response_id` to continue conversations\n",
    "- **Unified experience**: Combines the best of Chat Completions and Assistants APIs\n",
    "- **Stateless design**: Instructions are stateless and apply only to the current request\n",
    "- **Better performance**: More efficient conversation management\n",
    "- **Same powerful tools**: File Search and Code Interpreter are both fully supported\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "By the end of this tutorial, you'll be able to:\n",
    "\n",
    "1. Use the **File Search** tool to build knowledge-based assistants\n",
    "2. Leverage the **Code Interpreter** tool for data analysis and visualization\n",
    "3. Manage vector stores for document retrieval\n",
    "4. Chain responses to maintain conversation context\n",
    "5. Combine multiple tools for powerful AI applications\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8325b056",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up our environment with the required imports and initialize our OpenAI client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc07dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae86af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "print(\"OpenAI client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c827566d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: File Search with the Responses API\n",
    "\n",
    "File Search allows your AI to access knowledge from documents you provide. OpenAI automatically handles:\n",
    "- Parsing and chunking documents\n",
    "- Creating and storing embeddings\n",
    "- Performing vector and keyword search\n",
    "- Retrieving relevant content to answer queries\n",
    "\n",
    "### Understanding Vector Stores\n",
    "\n",
    "**Vector Stores** are specialized databases for efficient storage and retrieval of information:\n",
    "- Files are automatically processed (parsed, chunked, embedded)\n",
    "- Support both keyword and semantic search\n",
    "- Can store up to 10,000 files each\n",
    "- Maximum file size: 512 MB per file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995e7ca7",
   "metadata": {},
   "source": [
    "### Creating a Vector Store\n",
    "\n",
    "Let's create a vector store for our documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d47ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(store_name: str):\n",
    "    \"\"\"Creates a vector store for document storage.\"\"\"\n",
    "    vector_store = client.vector_stores.create(name=store_name)\n",
    "    \n",
    "    details = {\n",
    "        \"id\": vector_store.id,\n",
    "        \"name\": vector_store.name,\n",
    "        \"created_at\": vector_store.created_at,\n",
    "        \"file_count\": vector_store.file_counts.completed\n",
    "    }\n",
    "    \n",
    "    print(f\"Created vector store: {vector_store.name}\")\n",
    "    print(f\"Vector Store ID: {vector_store.id}\")\n",
    "    \n",
    "    return details\n",
    "\n",
    "# Create our first vector store\n",
    "vector_store_details = create_vector_store(\"Product Documentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f3311b",
   "metadata": {},
   "source": [
    "### Uploading Files to the Vector Store\n",
    "\n",
    "Now let's upload documents to our vector store. We'll download a sample research paper from arXiv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6250be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Download a sample research paper from arXiv\n",
    "paper_url = \"https://arxiv.org/pdf/2503.03718\"\n",
    "file_path = \"arxiv_paper_sample.pdf\"\n",
    "\n",
    "print(\"Downloading research paper...\")\n",
    "response = requests.get(paper_url)\n",
    "response.raise_for_status()\n",
    "\n",
    "with open(file_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "print(f\"Downloaded: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799f037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_to_vector_store(file_path: str, vector_store_id: str):\n",
    "    \"\"\"Uploads a file to the vector store.\"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    try:\n",
    "        # Upload file to OpenAI\n",
    "        print(f\"Uploading {file_name}...\")\n",
    "        file_response = client.files.create(\n",
    "            file=open(file_path, 'rb'),\n",
    "            purpose=\"assistants\"\n",
    "        )\n",
    "        \n",
    "        # Add file to vector store\n",
    "        print(f\"Adding to vector store...\")\n",
    "        client.vector_stores.files.create(\n",
    "            vector_store_id=vector_store_id,\n",
    "            file_id=file_response.id\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Successfully uploaded: {file_name}\")\n",
    "        return {\"file\": file_name, \"status\": \"success\", \"file_id\": file_response.id}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to upload {file_name}: {str(e)}\")\n",
    "        return {\"file\": file_name, \"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "# Upload our file\n",
    "upload_result = upload_file_to_vector_store(file_path, vector_store_details['id'])\n",
    "\n",
    "# Wait a moment for processing\n",
    "print(\"\\nWaiting for file processing...\")\n",
    "time.sleep(3)\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafedaae",
   "metadata": {},
   "source": [
    "### Using File Search with the Responses API\n",
    "\n",
    "Now comes the exciting part! With the Responses API, we can query our documents directly without managing threads or assistants.\n",
    "\n",
    "The key difference from the old Assistants API:\n",
    "- **Old way**: Create assistant → Create thread → Add message → Run assistant → Poll for completion\n",
    "- **New way**: Simply call `responses.create()` with the file_search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146f0cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_documents(query: str, vector_store_id: str, model: str = \"gpt-4o\"):\n",
    "    \"\"\"Query documents using file search in the Responses API.\"\"\"\n",
    "    \n",
    "    instructions = \"\"\"You are a helpful research assistant. \n",
    "    Use the provided documentation to answer questions accurately.\n",
    "    If you're not sure about something, admit it and stick to the information in the documents.\n",
    "    Always cite your sources when possible.\"\"\"\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        input=query,\n",
    "        model=model,\n",
    "        instructions=instructions,\n",
    "        tools=[{\n",
    "            \"type\": \"file_search\",\n",
    "            \"vector_store_ids\": [vector_store_id],\n",
    "            \"max_num_results\": 5\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Ask a question about our document\n",
    "query = \"What is this paper about? Provide a brief summary.\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "response = query_documents(query, vector_store_details['id'])\n",
    "\n",
    "# Display the response\n",
    "print(\"Assistant's Response:\")\n",
    "print(\"=\"*80)\n",
    "for item in response.output:\n",
    "    if hasattr(item, 'content'):\n",
    "        for content in item.content:\n",
    "            if hasattr(content, 'text'):\n",
    "                print(content.text)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a0350a",
   "metadata": {},
   "source": [
    "### Continuing the Conversation\n",
    "\n",
    "One of the most powerful features of the Responses API is simple conversation continuity. Just pass the `previous_response_id` to maintain context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b9586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_conversation(query: str, previous_response_id: str, vector_store_id: str, model: str = \"gpt-4o\"):\n",
    "    \"\"\"Continue a conversation using previous_response_id.\"\"\"\n",
    "    \n",
    "    instructions = \"\"\"You are a helpful research assistant. \n",
    "    Use the provided documentation to answer questions accurately.\n",
    "    Refer to our previous conversation when relevant.\"\"\"\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        input=query,\n",
    "        model=model,\n",
    "        instructions=instructions,\n",
    "        previous_response_id=previous_response_id,\n",
    "        tools=[{\n",
    "            \"type\": \"file_search\",\n",
    "            \"vector_store_ids\": [vector_store_id],\n",
    "            \"max_num_results\": 5\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Ask a follow-up question\n",
    "follow_up_query = \"Can you explain the methodology in more detail?\"\n",
    "print(f\"Follow-up Query: {follow_up_query}\\n\")\n",
    "\n",
    "follow_up_response = continue_conversation(\n",
    "    follow_up_query,\n",
    "    response.id,\n",
    "    vector_store_details['id']\n",
    ")\n",
    "\n",
    "# Display the response\n",
    "print(\"Assistant's Response:\")\n",
    "print(\"=\"*80)\n",
    "for item in follow_up_response.output:\n",
    "    if hasattr(item, 'content'):\n",
    "        for content in item.content:\n",
    "            if hasattr(content, 'text'):\n",
    "                print(content.text)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8e3712",
   "metadata": {},
   "source": [
    "### Extracting Citations\n",
    "\n",
    "The Responses API provides annotations that include information about which files were used to generate the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bae529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_citations(response):\n",
    "    \"\"\"Extract file citations from response annotations.\"\"\"\n",
    "    citations = set()\n",
    "    \n",
    "    for item in response.output:\n",
    "        if hasattr(item, 'content'):\n",
    "            for content in item.content:\n",
    "                if hasattr(content, 'annotations'):\n",
    "                    for annotation in content.annotations:\n",
    "                        if hasattr(annotation, 'filename'):\n",
    "                            citations.add(annotation.filename)\n",
    "    \n",
    "    return citations\n",
    "\n",
    "# Extract and display citations\n",
    "citations = extract_citations(follow_up_response)\n",
    "\n",
    "if citations:\n",
    "    print(\"\\nFiles Referenced:\")\n",
    "    for citation in citations:\n",
    "        print(f\"  - {citation}\")\n",
    "else:\n",
    "    print(\"\\nNo explicit file citations found in response.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113be6a2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Code Interpreter with the Responses API\n",
    "\n",
    "The Code Interpreter tool enables your AI to write and execute Python code in a secure, sandboxed environment. This is perfect for:\n",
    "- Data analysis and statistics\n",
    "- Creating visualizations and charts\n",
    "- Processing various file formats (CSV, JSON, Excel, etc.)\n",
    "- Solving complex mathematical problems\n",
    "- Generating files with data and results\n",
    "\n",
    "**Cost**: $0.03 per session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53a5abf",
   "metadata": {},
   "source": [
    "### Basic Code Interpreter Usage\n",
    "\n",
    "Let's start with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a5afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_code_interpreter(query: str, model: str = \"gpt-4o\"):\n",
    "    \"\"\"Use code interpreter to solve problems.\"\"\"\n",
    "    \n",
    "    instructions = \"\"\"You are a skilled data analyst and programmer.\n",
    "    When solving problems:\n",
    "    1. Write clear, well-documented code\n",
    "    2. Explain your approach\n",
    "    3. Show your work step by step\n",
    "    4. Provide insights from the results\"\"\"\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        input=query,\n",
    "        model=model,\n",
    "        instructions=instructions,\n",
    "        tools=[{\n",
    "            \"type\": \"code_interpreter\",\n",
    "            \"container\": {\"type\": \"auto\"}\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example: Solve a math problem\n",
    "math_query = \"\"\"Calculate the first 10 Fibonacci numbers and plot them on a graph. \n",
    "Also calculate the ratio between consecutive numbers and show how it approaches the golden ratio.\"\"\"\n",
    "\n",
    "print(f\"Query: {math_query}\\n\")\n",
    "\n",
    "math_response = run_code_interpreter(math_query)\n",
    "\n",
    "# Display the response\n",
    "print(\"Assistant's Response:\")\n",
    "print(\"=\"*80)\n",
    "for item in math_response.output:\n",
    "    if hasattr(item, 'content'):\n",
    "        for content in item.content:\n",
    "            if hasattr(content, 'text'):\n",
    "                print(content.text)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a42a65a",
   "metadata": {},
   "source": [
    "### Data Analysis with File Upload\n",
    "\n",
    "Let's create a sample dataset and analyze it using the Code Interpreter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca1dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2024-01-01', periods=100, freq='D')\n",
    "data = {\n",
    "    'date': dates,\n",
    "    'sales': np.random.randint(100, 1000, 100) + np.arange(100) * 5,\n",
    "    'expenses': np.random.randint(50, 500, 100) + np.arange(100) * 2,\n",
    "    'customers': np.random.randint(10, 100, 100)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "csv_file = \"sample_business_data.csv\"\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"Created sample dataset with {len(df)} rows\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc8bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data_file(file_path: str, query: str, model: str = \"gpt-4o\"):\n",
    "    \"\"\"Upload a file and analyze it using code interpreter.\"\"\"\n",
    "    \n",
    "    # Upload the file\n",
    "    file_response = client.files.create(\n",
    "        file=open(file_path, 'rb'),\n",
    "        purpose=\"assistants\"\n",
    "    )\n",
    "    \n",
    "    instructions = \"\"\"You are an expert data analyst.\n",
    "    Analyze the provided data file and create insightful visualizations.\n",
    "    Explain your findings clearly with statistical evidence.\"\"\"\n",
    "    \n",
    "    # Create input with file attachment\n",
    "    input_data = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_file\",\n",
    "                    \"file_id\": file_response.id\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": query\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        input=input_data,\n",
    "        model=model,\n",
    "        instructions=instructions,\n",
    "        tools=[{\n",
    "            \"type\": \"code_interpreter\",\n",
    "            \"container\": {\"type\": \"auto\"}\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    return response, file_response.id\n",
    "\n",
    "# Analyze our dataset\n",
    "analysis_query = \"\"\"Please analyze this business data:\n",
    "1. Calculate basic statistics (mean, median, trends)\n",
    "2. Create visualizations showing sales and expenses over time\n",
    "3. Calculate the profit margin trend\n",
    "4. Identify any interesting patterns or anomalies\n",
    "5. Provide actionable insights\"\"\"\n",
    "\n",
    "print(f\"Analyzing data...\\n\")\n",
    "\n",
    "analysis_response, uploaded_file_id = analyze_data_file(csv_file, analysis_query)\n",
    "\n",
    "# Display the response\n",
    "print(\"Analysis Results:\")\n",
    "print(\"=\"*80)\n",
    "for item in analysis_response.output:\n",
    "    if hasattr(item, 'content'):\n",
    "        for content in item.content:\n",
    "            if hasattr(content, 'text'):\n",
    "                print(content.text)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9be408",
   "metadata": {},
   "source": [
    "### Handling Generated Files\n",
    "\n",
    "Code Interpreter can generate files (like images, CSV files, etc.). Here's how to download them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c101f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_generated_files(response, output_dir=\"output\"):\n",
    "    \"\"\"Download any files generated by Code Interpreter.\"\"\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    downloaded_files = []\n",
    "    \n",
    "    for item in response.output:\n",
    "        if hasattr(item, 'content'):\n",
    "            for content in item.content:\n",
    "                # Check for image files\n",
    "                if hasattr(content, 'image_file'):\n",
    "                    file_id = content.image_file.file_id\n",
    "                    file_content = client.files.content(file_id)\n",
    "                    \n",
    "                    output_path = os.path.join(output_dir, f\"image_{file_id}.png\")\n",
    "                    with open(output_path, 'wb') as f:\n",
    "                        f.write(file_content.read())\n",
    "                    \n",
    "                    downloaded_files.append(output_path)\n",
    "                    print(f\"Downloaded: {output_path}\")\n",
    "                \n",
    "                # Check for file path annotations (other file types)\n",
    "                if hasattr(content, 'annotations'):\n",
    "                    for annotation in content.annotations:\n",
    "                        if hasattr(annotation, 'file_path'):\n",
    "                            file_id = annotation.file_path.file_id\n",
    "                            file_content = client.files.content(file_id)\n",
    "                            \n",
    "                            output_path = os.path.join(output_dir, f\"file_{file_id}\")\n",
    "                            with open(output_path, 'wb') as f:\n",
    "                                f.write(file_content.read())\n",
    "                            \n",
    "                            downloaded_files.append(output_path)\n",
    "                            print(f\"Downloaded: {output_path}\")\n",
    "    \n",
    "    return downloaded_files\n",
    "\n",
    "# Download any generated files from our analysis\n",
    "print(\"\\nChecking for generated files...\")\n",
    "files = download_generated_files(analysis_response)\n",
    "\n",
    "if files:\n",
    "    print(f\"\\nDownloaded {len(files)} file(s)\")\n",
    "else:\n",
    "    print(\"\\nNo files were generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fb5b1d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Combining File Search and Code Interpreter\n",
    "\n",
    "One of the most powerful features is combining both tools. You can search through documents AND perform data analysis in the same request!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6816be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_analysis(query: str, vector_store_id: str, model: str = \"gpt-4o\"):\n",
    "    \"\"\"Use both file_search and code_interpreter together.\"\"\"\n",
    "    \n",
    "    instructions = \"\"\"You are an advanced research and data analysis assistant.\n",
    "    Use file_search to find relevant information from documents.\n",
    "    Use code_interpreter to perform calculations, analysis, and create visualizations.\n",
    "    Combine insights from both tools to provide comprehensive answers.\"\"\"\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        input=query,\n",
    "        model=model,\n",
    "        instructions=instructions,\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"file_search\",\n",
    "                \"vector_store_ids\": [vector_store_id],\n",
    "                \"max_num_results\": 5\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"code_interpreter\",\n",
    "                \"container\": {\"type\": \"auto\"}\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example query that benefits from both tools\n",
    "combined_query = \"\"\"Based on the research paper in the knowledge base, \n",
    "extract any numerical data or statistics mentioned, and create a visualization \n",
    "to illustrate the key findings. Explain what the data shows.\"\"\"\n",
    "\n",
    "print(f\"Query: {combined_query}\\n\")\n",
    "\n",
    "combined_response = combined_analysis(combined_query, vector_store_details['id'])\n",
    "\n",
    "# Display the response\n",
    "print(\"Combined Analysis:\")\n",
    "print(\"=\"*80)\n",
    "for item in combined_response.output:\n",
    "    if hasattr(item, 'content'):\n",
    "        for content in item.content:\n",
    "            if hasattr(content, 'text'):\n",
    "                print(content.text)\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Download any generated files\n",
    "print(\"\\nChecking for generated visualizations...\")\n",
    "download_generated_files(combined_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75e0e49",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices and Cost Management\n",
    "\n",
    "### Cost Considerations\n",
    "\n",
    "- **File Search**: $2.50 per 1,000 queries + $0.10/GB/day storage (first GB free)\n",
    "- **Code Interpreter**: $0.03 per session\n",
    "- **Vector Storage**: Set expiration policies to manage costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store_with_expiration(name: str, days: int = 7):\n",
    "    \"\"\"Create a vector store with automatic expiration after inactivity.\"\"\"\n",
    "    \n",
    "    vector_store = client.vector_stores.create(\n",
    "        name=name,\n",
    "        expires_after={\n",
    "            \"anchor\": \"last_active_at\",\n",
    "            \"days\": days\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Created vector store: {name}\")\n",
    "    print(f\"Will expire after {days} days of inactivity\")\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "# Example: Create a temporary vector store\n",
    "temp_store = create_vector_store_with_expiration(\"Temporary Research Store\", days=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6ff4b",
   "metadata": {},
   "source": [
    "### Supported File Types\n",
    "\n",
    "File Search supports many text-based formats:\n",
    "- Documents: `.pdf`, `.docx`, `.txt`, `.md`\n",
    "- Code: `.py`, `.js`, `.java`, `.cpp`, `.html`, `.css`\n",
    "- Data: `.json`, `.csv`, `.xml`\n",
    "- Maximum file size: 512 MB\n",
    "- Maximum tokens per file: 5,000,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08739f47",
   "metadata": {},
   "source": [
    "### Cleanup Resources\n",
    "\n",
    "Remember to clean up resources when you're done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c30ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_resources(vector_store_ids=None, file_ids=None):\n",
    "    \"\"\"Clean up vector stores and files.\"\"\"\n",
    "    \n",
    "    if vector_store_ids:\n",
    "        for vs_id in vector_store_ids:\n",
    "            try:\n",
    "                client.vector_stores.delete(vs_id)\n",
    "                print(f\"Deleted vector store: {vs_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting vector store {vs_id}: {e}\")\n",
    "    \n",
    "    if file_ids:\n",
    "        for file_id in file_ids:\n",
    "            try:\n",
    "                client.files.delete(file_id)\n",
    "                print(f\"Deleted file: {file_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting file {file_id}: {e}\")\n",
    "\n",
    "# Example cleanup (uncomment to use)\n",
    "# cleanup_resources(\n",
    "#     vector_store_ids=[vector_store_details['id']],\n",
    "#     file_ids=[uploaded_file_id]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100cad61",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Migration Notes: Assistants API → Responses API\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Aspect | Assistants API (Old) | Responses API (New) |\n",
    "|--------|---------------------|--------------------|\n",
    "| **Conversation Management** | Thread-based, manual history | `previous_response_id` |\n",
    "| **Instructions** | Stored on server | Stateless, per-request |\n",
    "| **API Calls** | Create assistant → thread → message → run | Single `responses.create()` |\n",
    "| **Execution** | Asynchronous with polling | Synchronous or streaming |\n",
    "| **Complexity** | Higher (multiple objects) | Lower (unified) |\n",
    "\n",
    "### Code Comparison\n",
    "\n",
    "**Old Way (Assistants API):**\n",
    "```python\n",
    "# Create assistant\n",
    "assistant = client.beta.assistants.create(\n",
    "    instructions=\"You are helpful\",\n",
    "    tools=[{\"type\": \"file_search\"}]\n",
    ")\n",
    "\n",
    "# Create thread\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "# Add message\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"Hello\"\n",
    ")\n",
    "\n",
    "# Run assistant\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "# Get messages\n",
    "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "```\n",
    "\n",
    "**New Way (Responses API):**\n",
    "```python\n",
    "# Single call\n",
    "response = client.responses.create(\n",
    "    input=\"Hello\",\n",
    "    instructions=\"You are helpful\",\n",
    "    tools=[{\"type\": \"file_search\", \"vector_store_ids\": [vs_id]}]\n",
    ")\n",
    "\n",
    "# Continue conversation\n",
    "response2 = client.responses.create(\n",
    "    input=\"Follow up\",\n",
    "    previous_response_id=response.id,\n",
    "    tools=[{\"type\": \"file_search\", \"vector_store_ids\": [vs_id]}]\n",
    ")\n",
    "```\n",
    "\n",
    "### Benefits of Responses API\n",
    "\n",
    "1. **Simpler code**: Fewer API calls and objects to manage\n",
    "2. **Better performance**: Direct response without polling\n",
    "3. **Flexible state**: Choose stateful or stateless as needed\n",
    "4. **Easier debugging**: Unified request/response structure\n",
    "5. **Better typing**: Improved TypeScript definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4a6679",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Congratulations! You've learned how to use OpenAI's modern Responses API with both File Search and Code Interpreter tools.\n",
    "\n",
    "### What We Covered\n",
    "\n",
    "1. **File Search**: Build knowledge-based assistants that can search through documents\n",
    "2. **Code Interpreter**: Perform data analysis, create visualizations, and solve complex problems\n",
    "3. **Conversation Management**: Maintain context using `previous_response_id`\n",
    "4. **Combined Tools**: Use multiple tools together for powerful applications\n",
    "5. **Best Practices**: Cost management, file handling, and resource cleanup\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with your own documents and datasets\n",
    "- Combine tools creatively for your specific use cases\n",
    "- Explore the other tools available in the Responses API (web search, etc.)\n",
    "- Build production applications with proper error handling and monitoring\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [OpenAI Responses API Documentation](https://platform.openai.com/docs/api-reference/responses)\n",
    "- [File Search Guide](https://platform.openai.com/docs/guides/tools-file-search)\n",
    "- [OpenAI Cookbook Examples](https://cookbook.openai.com/)\n",
    "\n",
    "Happy building! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
