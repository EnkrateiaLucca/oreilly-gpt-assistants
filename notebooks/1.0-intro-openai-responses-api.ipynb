{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa885468",
   "metadata": {},
   "source": [
    "# Introduction to OpenAI Responses API: Building a Research Assistant\n",
    "\n",
    "## Overview\n",
    "In this notebook, we'll explore the OpenAI Responses API by building a practical research assistant that can help analyze academic papers and generate research summaries. The Responses API is the modern replacement for the deprecated Assistants API, offering improved performance and a simpler, more intuitive interface.\n",
    "\n",
    "The Responses API allows us to create AI-powered responses with specific instructions, capabilities, and access to various tools. In this lesson, we'll learn about:\n",
    "- Creating responses with custom instructions\n",
    "- Managing persistent conversations\n",
    "- Handling streaming responses\n",
    "- Working with built-in tools like Code Interpreter\n",
    "- Understanding the migration from Assistants to Responses API\n",
    "\n",
    "## Key Concept Changes from Assistants API\n",
    "\n",
    "If you're familiar with the Assistants API, here are the main changes:\n",
    "- **Assistants → Instructions**: No more persistent assistant objects. Instructions are passed with each request.\n",
    "- **Threads → Conversations**: Use `previous_response_id` to link responses together.\n",
    "- **Runs → Responses**: Direct, synchronous execution instead of async polling.\n",
    "- **Run Steps → Items**: Simplified streaming events.\n",
    "\n",
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea78a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8773e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c6d2f7",
   "metadata": {},
   "source": [
    "## What is the Responses API?\n",
    "\n",
    "The Responses API is OpenAI's modern approach to building conversational AI applications. It provides a streamlined way to create AI-powered responses with persistent conversation management and tool integration.\n",
    "\n",
    "### Key Features:\n",
    "1. **Stateless Instructions**:\n",
    "   - Define how the AI behaves by passing instructions with each request.\n",
    "   - No need to create and manage persistent assistant objects.\n",
    "\n",
    "2. **Tool Integration**:\n",
    "   - Leverage built-in tools such as:\n",
    "     - **Code Interpreter**: Execute and debug Python code.\n",
    "     - **File Search**: Search through uploaded documents.\n",
    "     - **Web Search**: Access real-time information from the internet.\n",
    "\n",
    "3. **Persistent Conversations**:\n",
    "   - Link responses together using `previous_response_id`.\n",
    "   - Automatically manage conversation context.\n",
    "   - Responses are stored for 30 days by default.\n",
    "\n",
    "4. **Performance Benefits**:\n",
    "   - 40-80% improvement over previous APIs through enhanced caching.\n",
    "   - Synchronous execution for simpler code.\n",
    "   - Real-time streaming support.\n",
    "\n",
    "### Core Concepts:\n",
    "- **Response**: A single AI-generated reply with a unique ID\n",
    "- **Conversation**: A chain of linked responses using `previous_response_id`\n",
    "- **Instructions**: System-level guidance for AI behavior (must be included with each request)\n",
    "- **Input**: The user's message or query\n",
    "- **Tools**: Built-in capabilities like code execution and file search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3d9b97",
   "metadata": {},
   "source": [
    "## Creating Our First Response\n",
    "\n",
    "Let's create a simple response with research-focused instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8cb321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define instructions for our research assistant\n",
    "research_instructions = \"\"\"You are a research assistant specialized in analyzing academic papers and research data.\n",
    "Your tasks include:\n",
    "- Summarizing research findings\n",
    "- Analyzing statistical data\n",
    "- Creating visualizations of research results\n",
    "- Providing critical analysis of methodologies\n",
    "Always maintain academic rigor and cite specific sections when referring to source materials.\"\"\"\n",
    "\n",
    "# Create our first response\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    instructions=research_instructions,\n",
    "    input=\"Hello! I'd like help analyzing some research data. Can you help?\"\n",
    ")\n",
    "\n",
    "print(f\"Response ID: {response.id}\")\n",
    "print(f\"\\nAssistant: {response.output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe60eec",
   "metadata": {},
   "source": [
    "## Managing Persistent Conversations\n",
    "\n",
    "To continue a conversation, we use the `previous_response_id` parameter. This automatically loads the conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4523a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the response ID for conversation continuity\n",
    "last_response_id = response.id\n",
    "\n",
    "# Continue the conversation\n",
    "response2 = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    instructions=research_instructions,\n",
    "    input=\"Great! I have patient recovery time data for two treatments that I'd like to compare.\",\n",
    "    previous_response_id=last_response_id\n",
    ")\n",
    "\n",
    "print(f\"Response ID: {response2.id}\")\n",
    "print(f\"\\nAssistant: {response2.output_text}\")\n",
    "\n",
    "# Update for next turn\n",
    "last_response_id = response2.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ff1b93",
   "metadata": {},
   "source": [
    "## Using Tools: Code Interpreter\n",
    "\n",
    "Let's enable the code interpreter tool to analyze statistical data. This allows the AI to execute Python code for data analysis and visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ec840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a response with code interpreter enabled\n",
    "analysis_response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    instructions=research_instructions,\n",
    "    input=\"\"\"I have a dataset of patient recovery times after two different treatments.\n",
    "Can you help me analyze the statistical significance of the results?\n",
    "\n",
    "Treatment A: [45, 42, 39, 47, 41, 43, 40, 44, 38, 46]\n",
    "Treatment B: [52, 49, 51, 47, 50, 53, 48, 51, 49, 50]\"\"\",\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    previous_response_id=last_response_id,\n",
    "    store=True  # Store the response for 30 days\n",
    ")\n",
    "\n",
    "print(f\"Response ID: {analysis_response.id}\")\n",
    "print(f\"\\nAssistant: {analysis_response.output_text}\")\n",
    "\n",
    "# Update for next turn\n",
    "last_response_id = analysis_response.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdad5ad",
   "metadata": {},
   "source": [
    "## Streaming Responses\n",
    "\n",
    "For a better user experience, we can stream responses in real-time. Let's implement a streaming handler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a streaming response\n",
    "print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "stream = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    instructions=research_instructions,\n",
    "    input=\"\"\"Can you create visualizations comparing the two treatments and \n",
    "perform a two-sample t-test to determine statistical significance?\n",
    "\n",
    "Treatment A: [45, 42, 39, 47, 41, 43, 40, 44, 38, 46]\n",
    "Treatment B: [52, 49, 51, 47, 50, 53, 48, 51, 49, 50]\"\"\",\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Process the stream\n",
    "current_response_id = None\n",
    "for event in stream:\n",
    "    # Handle text deltas\n",
    "    if event.type == \"response.output_text.delta\":\n",
    "        print(event.delta, end=\"\", flush=True)\n",
    "    \n",
    "    # Handle tool calls\n",
    "    elif event.type == \"response.output_item.added\":\n",
    "        if hasattr(event, 'item') and event.item.type == \"code_interpreter\":\n",
    "            print(\"\\n\\n[Running code analysis...]\\n\", flush=True)\n",
    "    \n",
    "    # Capture the response ID\n",
    "    elif event.type == \"response.completed\":\n",
    "        current_response_id = event.response.id\n",
    "        print(f\"\\n\\nResponse ID: {current_response_id}\")\n",
    "\n",
    "# Update for conversation continuity\n",
    "if current_response_id:\n",
    "    last_response_id = current_response_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86732200",
   "metadata": {},
   "source": [
    "## Advanced Streaming with Event Handling\n",
    "\n",
    "Let's create a more sophisticated event handler for better control over streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced_stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_streaming_response(stream):\n",
    "    \"\"\"Process streaming response events with detailed handling.\"\"\"\n",
    "    response_id = None\n",
    "    text_buffer = []\n",
    "    \n",
    "    for event in stream:\n",
    "        # Text content streaming\n",
    "        if event.type == \"response.output_text.delta\":\n",
    "            print(event.delta, end=\"\", flush=True)\n",
    "            text_buffer.append(event.delta)\n",
    "        \n",
    "        # New item added (like code execution)\n",
    "        elif event.type == \"response.output_item.added\":\n",
    "            item_type = getattr(event.item, 'type', 'unknown')\n",
    "            print(f\"\\n\\n[Tool: {item_type}]\\n\", flush=True)\n",
    "        \n",
    "        # Code interpreter input\n",
    "        elif event.type == \"response.output_item.code_interpreter.input\":\n",
    "            print(\"\\nCode Input:\", flush=True)\n",
    "            print(event.input, flush=True)\n",
    "        \n",
    "        # Code interpreter logs\n",
    "        elif event.type == \"response.output_item.code_interpreter.logs\":\n",
    "            print(\"\\nCode Output:\", flush=True)\n",
    "            print(event.logs, flush=True)\n",
    "        \n",
    "        # Response completed\n",
    "        elif event.type == \"response.completed\":\n",
    "            response_id = event.response.id\n",
    "            print(f\"\\n\\n✓ Response completed (ID: {response_id})\")\n",
    "    \n",
    "    return response_id, ''.join(text_buffer)\n",
    "\n",
    "# Use the handler\n",
    "print(\"\\nAssistant: \", end=\"\", flush=True)\n",
    "stream = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    instructions=research_instructions,\n",
    "    input=\"Please calculate the mean and standard deviation for both treatment groups.\",\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    previous_response_id=last_response_id,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "response_id, full_text = handle_streaming_response(stream)\n",
    "last_response_id = response_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8be1ef",
   "metadata": {},
   "source": [
    "## Retrieving Past Responses\n",
    "\n",
    "You can retrieve previously stored responses using their IDs. Responses are stored for 30 days by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8127e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a previous response\n",
    "if last_response_id:\n",
    "    retrieved_response = client.responses.retrieve(last_response_id)\n",
    "    \n",
    "    print(f\"Retrieved Response ID: {retrieved_response.id}\")\n",
    "    print(f\"Model: {retrieved_response.model}\")\n",
    "    print(f\"Created: {retrieved_response.created_at}\")\n",
    "    print(f\"\\nOutput: {retrieved_response.output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25239107",
   "metadata": {},
   "source": [
    "## Practice Exercise\n",
    "\n",
    "Now it's your turn! Try creating a response for a different use case. Here's a template to get started:\n",
    "\n",
    "**Helpful Resources:**\n",
    "1. [OpenAI API Reference - Responses](https://platform.openai.com/docs/api-reference/responses/create)\n",
    "2. [Migration Guide from Assistants to Responses API](https://platform.openai.com/docs/assistants/migration)\n",
    "3. [OpenAI API Quickstart](https://platform.openai.com/docs/quickstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab1c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your custom response\n",
    "\n",
    "# 1. Define your instructions\n",
    "custom_instructions = \"\"\"[Your detailed instructions here]\n",
    "For example:\n",
    "- You are a financial analyst assistant\n",
    "- You help analyze investment portfolios\n",
    "- You provide data-driven recommendations\n",
    "\"\"\"\n",
    "\n",
    "# 2. Create your first response\n",
    "my_response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    instructions=custom_instructions,\n",
    "    input=\"[Your first question or request]\",\n",
    "    tools=[{\"type\": \"code_interpreter\"}],  # Add tools as needed\n",
    "    stream=True  # Enable streaming for real-time output\n",
    ")\n",
    "\n",
    "# 3. Process the response\n",
    "response_id, output = handle_streaming_response(my_response)\n",
    "\n",
    "# 4. Continue the conversation\n",
    "# follow_up = client.responses.create(\n",
    "#     model=\"gpt-4o\",\n",
    "#     instructions=custom_instructions,\n",
    "#     input=\"[Your follow-up question]\",\n",
    "#     previous_response_id=response_id\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conversation_tips",
   "metadata": {},
   "source": [
    "## Best Practices for Conversation Management\n",
    "\n",
    "### 1. **Always Include Instructions**\n",
    "Unlike the Assistants API, instructions are NOT persistent. You must include them with every request:\n",
    "```python\n",
    "# Good - Instructions included\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    instructions=my_instructions,  # ✓ Always include\n",
    "    input=\"Your message\",\n",
    "    previous_response_id=last_id\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. **Manage Conversation Context**\n",
    "Use `previous_response_id` to chain responses:\n",
    "```python\n",
    "# Start conversation\n",
    "response1 = client.responses.create(...)\n",
    "last_id = response1.id\n",
    "\n",
    "# Continue conversation\n",
    "response2 = client.responses.create(\n",
    "    ...,\n",
    "    previous_response_id=last_id\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. **Control Storage and Costs**\n",
    "```python\n",
    "# Store response for 30 days (default: True)\n",
    "response = client.responses.create(\n",
    "    ...,\n",
    "    store=True  # or False to disable storage\n",
    ")\n",
    "\n",
    "# Note: previous_response_id increases token costs\n",
    "# as it loads full conversation history\n",
    "```\n",
    "\n",
    "### 4. **Handle Long Conversations**\n",
    "```python\n",
    "# Enable automatic truncation for token management\n",
    "response = client.responses.create(\n",
    "    ...,\n",
    "    truncation=\"auto\"  # Automatically manages context length\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c01e23",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Responses API vs Assistants API\n",
    "\n",
    "| Feature | Assistants API (Deprecated) | Responses API (New) |\n",
    "|---------|---------------------------|---------------------|\n",
    "| **Execution Model** | Asynchronous polling | Synchronous |\n",
    "| **Configuration** | Persistent Assistant objects | Stateless instructions |\n",
    "| **Conversation** | Thread objects | `previous_response_id` |\n",
    "| **Performance** | Baseline | 40-80% faster |\n",
    "| **Complexity** | High (polling, status checks) | Low (direct responses) |\n",
    "| **Storage** | Built-in thread storage | Optional 30-day storage |\n",
    "\n",
    "### Key Concepts Learned\n",
    "\n",
    "1. **Stateless Design**: Instructions must be included with each request for consistent behavior.\n",
    "2. **Conversation Continuity**: Use `previous_response_id` to maintain context across multiple turns.\n",
    "3. **Synchronous Execution**: No more polling - responses are returned immediately or streamed in real-time.\n",
    "4. **Tool Integration**: Built-in tools like Code Interpreter work seamlessly with the API.\n",
    "5. **Performance Benefits**: Significant speed improvements through enhanced caching.\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "- **Single-turn interactions**: Simple `responses.create()` without `previous_response_id`\n",
    "- **Multi-turn conversations**: Chain responses using `previous_response_id`\n",
    "- **Real-time applications**: Enable `stream=True` for immediate feedback\n",
    "- **Data analysis**: Include `code_interpreter` tool for computational tasks\n",
    "- **Document search**: Use `file_search` tool for RAG applications\n",
    "- **Web access**: Enable `web_search_preview` for current information\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore additional tools (File Search, Web Search)\n",
    "- Implement custom function calling for external integrations\n",
    "- Build more complex conversation flows\n",
    "- Optimize token usage with truncation strategies\n",
    "- Experiment with different models (gpt-4o, gpt-4-turbo, etc.)\n",
    "\n",
    "## Migration Timeline\n",
    "\n",
    "- **March 2025**: Responses API launched\n",
    "- **Mid-2025**: Deprecation date announced\n",
    "- **August 26, 2026**: Assistants API sunset\n",
    "\n",
    "**Action Items:**\n",
    "- Begin migrating existing Assistants API code to Responses API\n",
    "- Update applications to use `previous_response_id` pattern\n",
    "- Test streaming implementations\n",
    "- Review token usage and optimize for new caching system\n",
    "\n",
    "Remember to handle your API keys securely and implement proper error handling in production environments!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
