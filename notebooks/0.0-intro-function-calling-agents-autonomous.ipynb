{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can you SEE MY SCREEN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\"> \n",
    "How do we connect LLMs with Tools?\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openai==1.59.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't have an API key as an environment variable you can set it here\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"var: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To connect Large Language Models (LLMs) with tools, you can use an interface that bridges the LLM's outputs with the APIs or functionalities of the tools, enabling seamless communication and interaction based on the model's generated responses.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def llm_response(prompt: str):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "llm_response(\"How do we connect LLMs with Tools? Output should be a single sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could ask the model to:\n",
    "\n",
    "1. Write Python code and then give the model the ability to run that code somewhere\n",
    "2. Create the Python function that executes the task, then somehow give the model the ability to run that function autonomously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option number 2 is what we call FUNCTION CALLING!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calling from Scratch\n",
    "\n",
    "1. Write a Python function\n",
    "2. Make that function \"available\" to the LLM model of choice (in this case will be gpt-4o/gpt-4o-mini)\n",
    "3. Test if the model can execute that function properly\n",
    "   1. Prepare the payload for the function (the arguments for it...)\n",
    "   2. Write the function call\n",
    "   3. We run the function call ourselves\n",
    "   4. We inspect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Folder created at ./pancakes-are-the-best'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_folder(folder_path: str):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    return f\"Folder created at {folder_path}\"\n",
    "\n",
    "create_folder(\"./pancakes-are-the-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36m./pancakes-are-the-best\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls -d ./* | grep pancakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'File created at ./pancakes-are-the-best/pancakes-are-the-best.txt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def write_file(file_path: str, content: str):\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(content)\n",
    "    return f\"File created at {file_path}\"\n",
    "\n",
    "write_file(\"./pancakes-are-the-best/pancakes-are-the-best.txt\", \"Pancakes are the best!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pancakes are the best!"
     ]
    }
   ],
   "source": [
    "!cat pancakes-are-the-best/pancakes-are-the-best.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM + Python Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most silly way possible first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_with_function_information = \"\"\"\n",
    "You are a personal assistant with desktop capabilities. \n",
    "Users will ask you to perform tasks and you will execute those tasks by calling one or more of the following functions:\n",
    "\n",
    "- create_folder(folder_path: str)\n",
    "- write_file(file_path: str, content: str)\n",
    "\n",
    "For example if the user asks: \n",
    "\n",
    "'Create a folder called lucas-teaches-function-callling'\n",
    "\n",
    "Your output should be code like this:\n",
    "'create_folder(\"./lucas-teaches-function-callling\")'\n",
    "\n",
    "'Create a folder called pancakes-are-the-best and write a file called pancakes-are-the-best.txt with the content \"Pancakes are the best!\"'\n",
    "\n",
    "Your output should be code like this:\n",
    "\n",
    "['create_folder(\"./pancakes-are-the-best\")', 'write_file(\"./pancakes-are-the-best/pancakes-are-the-best.txt\", \"Pancakes are the best!\")']\n",
    "\n",
    "Your OUTPUT should ALWAYS BE python code only.\n",
    "\n",
    "Here is the user prompt:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_prompt = \"Create a file named pancakes-are-better-than-waffles.md with a one paragraph summary for why pancakes are better, \\\n",
    "make sure to address EG and BS students from my course who dared to say waffles are better.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\nwrite_file(\"./pancakes-are-better-than-waffles.md\", \"Pancakes are undeniably better than waffles, and to my dear EG and BS students who dared to argue otherwise, let me clarify. The fluffy texture of pancakes, combined with their ability to absorb syrup, creates a harmonious blend of flavors, making them the perfect breakfast choice. Unlike waffles, which can be overly crispy and rigid, pancakes offer a comforting, soft experience that melts in your mouth. So let\\'s embrace the pancake revolution and celebrate the culinary delight they bring to our breakfast tables!\")\\n```'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_with_function_information + user_prompt\n",
    "output_with_python_code_and_ticks = llm_response(prompt)\n",
    "output_with_python_code_and_ticks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up this output a little bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write_file(\"./pancakes-are-better-than-waffles.md\", \"Pancakes are undeniably better than waffles, and to my dear EG and BS students who dared to argue otherwise, let me clarify. The fluffy texture of pancakes, combined with their ability to absorb syrup, creates a harmonious blend of flavors, making them the perfect breakfast choice. Unlike waffles, which can be overly crispy and rigid, pancakes offer a comforting, soft experience that melts in your mouth. So let\\'s embrace the pancake revolution and celebrate the culinary delight they bring to our breakfast tables!\")\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_cleaned = output_with_python_code_and_ticks.replace(\"```python\\n\", \"\").replace(\"```\", \"\")\n",
    "\n",
    "output_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find a way to execute this code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(output_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pancakes-are-better-than-waffles.md\n",
      "\u001b[1m\u001b[36mpancakes-are-the-best\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls | grep pancakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could improve this a little bit by writing up a couple of functions that do a slightly better job at cleaning up the output generated by the LLM,\n",
    "and then executing the resulting function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_llm_function_calls(llm_output: str):\n",
    "    \"\"\"\n",
    "    Naive parser that looks for function calls in the model output (e.g. 'create_folder(\"test\")')\n",
    "    and returns a list of dicts, each containing 'function_name' and 'args'.\n",
    "    \"\"\"\n",
    "    calls = []\n",
    "    \n",
    "    # Regex looks for function_name( ... ) \n",
    "    # capturing everything until the next matching parenthesis\n",
    "    pattern = r\"(\\w+)\\(([^)]*)\\)\"\n",
    "    matches = re.findall(pattern, llm_output)\n",
    "    \n",
    "    for match in matches:\n",
    "        function_name = match[0]\n",
    "        raw_args = match[1].split(',')\n",
    "        \n",
    "        # Clean up quotes/spaces\n",
    "        parsed_args = [arg.strip().strip('\"').strip(\"'\") for arg in raw_args]\n",
    "        calls.append({\n",
    "            \"function_name\": function_name,\n",
    "            \"args\": parsed_args\n",
    "        })\n",
    "    \n",
    "    return calls\n",
    "\n",
    "def execute_llm_function_calls(calls):\n",
    "    \"\"\"\n",
    "    Executes each call by mapping the function_name to an actual Python function.\n",
    "    If a function is unknown, handle gracefully.\n",
    "    \"\"\"\n",
    "    for call in calls:\n",
    "        fn_name = call[\"function_name\"]\n",
    "        args = call[\"args\"]\n",
    "        \n",
    "        if fn_name == \"create_folder\":\n",
    "            create_folder(*args)\n",
    "        elif fn_name == \"write_file\":\n",
    "            write_file(*args)\n",
    "        else:\n",
    "            print(f\"[Warning] Unknown function: {fn_name}\")\n",
    "\n",
    "# Example usage\n",
    "# Suppose the model returned the following text (as a single string):\n",
    "# 'create_folder(\"./some_folder\") write_file(\"./some_folder/readme.txt\",\"Some content\")'\n",
    "\n",
    "model_generated_code = \"\"\"\n",
    "create_folder(\"./some_folder\")\n",
    "write_file(\"./some_folder/readme.txt\", \"Some content\")\n",
    "\"\"\"\n",
    "\n",
    "# 1) Parse\n",
    "calls = parse_llm_function_calls(model_generated_code)\n",
    "\n",
    "# 2) Execute\n",
    "execute_llm_function_calls(calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readme.txt\n"
     ]
    }
   ],
   "source": [
    "!ls some_folder/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We connected LLM + FUNCTION via some simple PROMPT magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36msome_folder/\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls -d */ | grep some"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-assistants",
   "language": "python",
   "name": "gpt-assistants"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
