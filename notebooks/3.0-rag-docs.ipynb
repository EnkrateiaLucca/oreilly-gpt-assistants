{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5885d562",
   "metadata": {},
   "source": [
    "# Building AI Applications with OpenAI's Responses API\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to this comprehensive guide on using OpenAI's **Responses API**, the modern replacement for the deprecated Assistants API. In this tutorial, we'll explore how to build powerful AI applications that can search through documents and perform complex data analysis.\n",
    "\n",
    "### What's Changed?\n",
    "\n",
    "The Assistants API is being deprecated (sunset planned for mid-2026) in favor of the new **Responses API**, which offers:\n",
    "\n",
    "- **Simpler architecture**: No more complex thread management - just pass `previous_response_id` to continue conversations\n",
    "- **Unified experience**: Combines the best of Chat Completions and Assistants APIs\n",
    "- **Stateless design**: Instructions are stateless and apply only to the current request\n",
    "- **Better performance**: More efficient conversation management\n",
    "- **Same powerful tools**: File Search and Code Interpreter are both fully supported\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "By the end of this tutorial, you'll be able to:\n",
    "\n",
    "1. Use the **File Search** tool to build knowledge-based assistants\n",
    "2. Leverage the **Code Interpreter** tool for data analysis and visualization\n",
    "3. Manage vector stores for document retrieval\n",
    "4. Chain responses to maintain conversation context\n",
    "5. Combine multiple tools for powerful AI applications\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8325b056",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up our environment with the required imports and initialize our OpenAI client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5acc07dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dae86af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "print(\"OpenAI client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c827566d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: File Search with the Responses API\n",
    "\n",
    "File Search allows your AI to access knowledge from documents you provide. OpenAI automatically handles:\n",
    "- Parsing and chunking documents\n",
    "- Creating and storing embeddings\n",
    "- Performing vector and keyword search\n",
    "- Retrieving relevant content to answer queries\n",
    "\n",
    "### Understanding Vector Stores\n",
    "\n",
    "**Vector Stores** are specialized databases for efficient storage and retrieval of information:\n",
    "- Files are automatically processed (parsed, chunked, embedded)\n",
    "- Support both keyword and semantic search\n",
    "- Can store up to 10,000 files each\n",
    "- Maximum file size: 512 MB per file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995e7ca7",
   "metadata": {},
   "source": [
    "### Creating a Vector Store\n",
    "\n",
    "Let's create a vector store for our documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d47ed5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/httpx/_transports/default.py:72\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n\u001b[1;32m    238\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[39mraise\u001b[39;00m exc \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[39m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mhandle_request(\n\u001b[1;32m    237\u001b[0m         pool_request\u001b[39m.\u001b[39;49mrequest\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[39m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[39m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[39m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connect_failed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/httpcore/_sync/connection.py:78\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connect(request)\n\u001b[1;32m     80\u001b[0m     ssl_object \u001b[39m=\u001b[39m stream\u001b[39m.\u001b[39mget_extra_info(\u001b[39m\"\u001b[39m\u001b[39mssl_object\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/httpcore/_sync/connection.py:124\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39mconnect_tcp\u001b[39m\u001b[39m\"\u001b[39m, logger, request, kwargs) \u001b[39mas\u001b[39;00m trace:\n\u001b[0;32m--> 124\u001b[0m     stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_network_backend\u001b[39m.\u001b[39;49mconnect_tcp(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    125\u001b[0m     trace\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m stream\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/httpcore/_backends/sync.py:207\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    202\u001b[0m exc_map: ExceptionMapping \u001b[39m=\u001b[39m {\n\u001b[1;32m    203\u001b[0m     socket\u001b[39m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[1;32m    204\u001b[0m     \u001b[39mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    205\u001b[0m }\n\u001b[0;32m--> 207\u001b[0m \u001b[39mwith\u001b[39;49;00m map_exceptions(exc_map):\n\u001b[1;32m    208\u001b[0m     sock \u001b[39m=\u001b[39;49m socket\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    209\u001b[0m         address,\n\u001b[1;32m    210\u001b[0m         timeout,\n\u001b[1;32m    211\u001b[0m         source_address\u001b[39m=\u001b[39;49msource_address,\n\u001b[1;32m    212\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen\u001b[39m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    159\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[39mraise\u001b[39;00m to_exc(exc) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 8] nodename nor servname provided, or not known",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/openai/_base_client.py:982\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 982\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msend(\n\u001b[1;32m    983\u001b[0m         request,\n\u001b[1;32m    984\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_should_stream_response_body(request\u001b[39m=\u001b[39;49mrequest),\n\u001b[1;32m    985\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    986\u001b[0m     )\n\u001b[1;32m    987\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    924\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_auth(\n\u001b[1;32m    927\u001b[0m     request,\n\u001b[1;32m    928\u001b[0m     auth\u001b[39m=\u001b[39;49mauth,\n\u001b[1;32m    929\u001b[0m     follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    930\u001b[0m     history\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m    931\u001b[0m )\n\u001b[1;32m    932\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_redirects(\n\u001b[1;32m    955\u001b[0m         request,\n\u001b[1;32m    956\u001b[0m         follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    957\u001b[0m         history\u001b[39m=\u001b[39;49mhistory,\n\u001b[1;32m    958\u001b[0m     )\n\u001b[1;32m    959\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_single_request(request)\n\u001b[1;32m    992\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m   1029\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, SyncByteStream)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/httpx/_transports/default.py:235\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[0;32m--> 235\u001b[0m \u001b[39mwith\u001b[39;49;00m map_httpcore_exceptions():\n\u001b[1;32m    236\u001b[0m     resp \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen\u001b[39m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    159\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/httpx/_transports/default.py:89\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m message \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(exc)\n\u001b[0;32m---> 89\u001b[0m \u001b[39mraise\u001b[39;00m mapped_exc(message) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mexc\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 8] nodename nor servname provided, or not known",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-gpt-assistants/notebooks/3.0-rag-docs.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-gpt-assistants/notebooks/3.0-rag-docs.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m details\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-gpt-assistants/notebooks/3.0-rag-docs.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Create our first vector store\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-gpt-assistants/notebooks/3.0-rag-docs.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m vector_store_details \u001b[39m=\u001b[39m create_vector_store(\u001b[39m\"\u001b[39;49m\u001b[39mProduct Documentation\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-gpt-assistants/notebooks/3.0-rag-docs.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-gpt-assistants/notebooks/3.0-rag-docs.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcreate_vector_store\u001b[39m(store_name: \u001b[39mstr\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-gpt-assistants/notebooks/3.0-rag-docs.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Creates a vector store for document storage.\"\"\"\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-gpt-assistants/notebooks/3.0-rag-docs.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     vector_store \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mvector_stores\u001b[39m.\u001b[39;49mcreate(name\u001b[39m=\u001b[39;49mstore_name)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-gpt-assistants/notebooks/3.0-rag-docs.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     details \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-gpt-assistants/notebooks/3.0-rag-docs.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m: vector_store\u001b[39m.\u001b[39mid,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-gpt-assistants/notebooks/3.0-rag-docs.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: vector_store\u001b[39m.\u001b[39mname,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-gpt-assistants/notebooks/3.0-rag-docs.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcreated_at\u001b[39m\u001b[39m\"\u001b[39m: vector_store\u001b[39m.\u001b[39mcreated_at,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-gpt-assistants/notebooks/3.0-rag-docs.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfile_count\u001b[39m\u001b[39m\"\u001b[39m: vector_store\u001b[39m.\u001b[39mfile_counts\u001b[39m.\u001b[39mcompleted\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-gpt-assistants/notebooks/3.0-rag-docs.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-gpt-assistants/notebooks/3.0-rag-docs.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCreated vector store: \u001b[39m\u001b[39m{\u001b[39;00mvector_store\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/openai/resources/vector_stores/vector_stores.py:128\u001b[0m, in \u001b[0;36mVectorStores.create\u001b[0;34m(self, chunking_strategy, description, expires_after, file_ids, metadata, name, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mCreate a vector store.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m extra_headers \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mOpenAI-Beta\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39massistants=v2\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(extra_headers \u001b[39mor\u001b[39;00m {})}\n\u001b[0;32m--> 128\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    129\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m/vector_stores\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    130\u001b[0m     body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    131\u001b[0m         {\n\u001b[1;32m    132\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mchunking_strategy\u001b[39;49m\u001b[39m\"\u001b[39;49m: chunking_strategy,\n\u001b[1;32m    133\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mdescription\u001b[39;49m\u001b[39m\"\u001b[39;49m: description,\n\u001b[1;32m    134\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mexpires_after\u001b[39;49m\u001b[39m\"\u001b[39;49m: expires_after,\n\u001b[1;32m    135\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mfile_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m: file_ids,\n\u001b[1;32m    136\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m: metadata,\n\u001b[1;32m    137\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m\"\u001b[39;49m: name,\n\u001b[1;32m    138\u001b[0m         },\n\u001b[1;32m    139\u001b[0m         vector_store_create_params\u001b[39m.\u001b[39;49mVectorStoreCreateParams,\n\u001b[1;32m    140\u001b[0m     ),\n\u001b[1;32m    141\u001b[0m     options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    142\u001b[0m         extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    143\u001b[0m     ),\n\u001b[1;32m    144\u001b[0m     cast_to\u001b[39m=\u001b[39;49mVectorStore,\n\u001b[1;32m    145\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-assistants/lib/python3.11/site-packages/openai/_base_client.py:1014\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRaising connection error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1014\u001b[0m     \u001b[39mraise\u001b[39;00m APIConnectionError(request\u001b[39m=\u001b[39mrequest) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39merr\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m log\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m   1017\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mHTTP Response: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1018\u001b[0m     request\u001b[39m.\u001b[39mmethod,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     response\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mrequest_id: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, response\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mx-request-id\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m: Connection error."
     ]
    }
   ],
   "source": [
    "def create_vector_store(store_name: str):\n",
    "    \"\"\"Creates a vector store for document storage.\"\"\"\n",
    "    vector_store = client.vector_stores.create(name=store_name)\n",
    "    \n",
    "    details = {\n",
    "        \"id\": vector_store.id,\n",
    "        \"name\": vector_store.name,\n",
    "        \"created_at\": vector_store.created_at,\n",
    "        \"file_count\": vector_store.file_counts.completed\n",
    "    }\n",
    "    \n",
    "    print(f\"Created vector store: {vector_store.name}\")\n",
    "    print(f\"Vector Store ID: {vector_store.id}\")\n",
    "    \n",
    "    return details\n",
    "\n",
    "# Create our first vector store\n",
    "vector_store_details = create_vector_store(\"Product Documentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f3311b",
   "metadata": {},
   "source": [
    "### Uploading Files to the Vector Store\n",
    "\n",
    "Now let's upload documents to our vector store. We'll download a sample research paper from arXiv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6250be55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading research paper...\n",
      "Downloaded: ./pdfs/future_agents.pdf\n",
      "Downloading research paper...\n",
      "Downloaded: ./pdfs/attention_paper.pdf\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Download a sample research paper from arXiv\n",
    "\n",
    "def download_paper(paper_url: str, file_path: str):\n",
    "    \"\"\"Download a research paper from arXiv.\"\"\"\n",
    "    print(\"Downloading research paper...\")\n",
    "    response = requests.get(paper_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    print(f\"Downloaded: {file_path}\")\n",
    "    \n",
    "    return file_path\n",
    "\n",
    "\n",
    "\n",
    "file_paths = [\"./assets-resources/pdfs/future_agents.pdf\",\"./assets-resources/pdfs/attention_paper.pdf\"]\n",
    "\n",
    "paper_urls = [\n",
    "    \"https://arxiv.org/pdf/2506.02153\",\n",
    "    \"https://arxiv.org/pdf/1706.03762\"\n",
    "]\n",
    "\n",
    "for file_path, paper_url in zip(file_paths, paper_urls):\n",
    "    download_paper(paper_url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3799f037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading future_agents.pdf...\n",
      "Adding to vector store...\n",
      "✓ Successfully uploaded: future_agents.pdf\n",
      "\n",
      "Waiting for file processing...\n",
      "Ready!\n",
      "Uploading attention_paper.pdf...\n",
      "Adding to vector store...\n",
      "✓ Successfully uploaded: attention_paper.pdf\n",
      "\n",
      "Waiting for file processing...\n",
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "def upload_file_to_vector_store(file_path: str, vector_store_id: str):\n",
    "    \"\"\"Uploads a file to the vector store.\"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    try:\n",
    "        # Upload file to OpenAI\n",
    "        print(f\"Uploading {file_name}...\")\n",
    "        file_response = client.files.create(\n",
    "            file=open(file_path, 'rb'),\n",
    "            purpose=\"assistants\"\n",
    "        )\n",
    "        \n",
    "        # Add file to vector store\n",
    "        print(f\"Adding to vector store...\")\n",
    "        client.vector_stores.files.create(\n",
    "            vector_store_id=vector_store_id,\n",
    "            file_id=file_response.id\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Successfully uploaded: {file_name}\")\n",
    "        return {\"file\": file_name, \"status\": \"success\", \"file_id\": file_response.id}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to upload {file_name}: {str(e)}\")\n",
    "        return {\"file\": file_name, \"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "# Upload our file\n",
    "# Created this vector store below manually in the openai platform\n",
    "# vector_store_id = vector_store_details['id']\n",
    "vector_store_id = \"vs_6900b46abc748191b31df0c8d561e5d7\"\n",
    "\n",
    "for file_path in file_paths:\n",
    "    upload_result = upload_file_to_vector_store(file_path, vector_store_id)\n",
    "    # Wait a moment for processing\n",
    "    print(\"\\nWaiting for file processing...\")\n",
    "    time.sleep(3)\n",
    "    print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafedaae",
   "metadata": {},
   "source": [
    "### Using File Search with the Responses API\n",
    "\n",
    "Now comes the exciting part! With the Responses API, we can query our documents directly without managing threads or assistants.\n",
    "\n",
    "The key difference from the old Assistants API:\n",
    "- **Old way**: Create assistant → Create thread → Add message → Run assistant → Poll for completion\n",
    "- **New way**: Simply call `responses.create()` with the file_search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "146f0cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Explain the core idea behind the attention mechanism and what is the future of agents according to the papers you have access to.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def query_documents(query: str, vector_store_id: str, model: str = \"gpt-5-mini\"):\n",
    "    \"\"\"Query documents using file search in the Responses API.\"\"\"\n",
    "    \n",
    "    instructions = \"\"\"You are a helpful research assistant. \n",
    "    Use the provided documentation to answer questions accurately.\n",
    "    If you're not sure about something, admit it and stick to the information in the documents.\n",
    "    Always cite your sources when possible.\"\"\"\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        input=query,\n",
    "        model=model,\n",
    "        instructions=instructions,\n",
    "        tools=[{\n",
    "            \"type\": \"file_search\",\n",
    "            \"vector_store_ids\": [vector_store_id],\n",
    "            \"max_num_results\": 5\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "# vector_store_id = vector_store_details['id']\n",
    "# Ask a question about our document\n",
    "query = \"Explain the core idea behind the attention mechanism and what is the future of agents according to the papers you have access to.\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "response = query_documents(query, vector_store_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a511350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Short answer first:\n",
       "\n",
       "- Attention: the core idea is to let the model compute, for each position (or query), a weighted sum of (value) representations where the weights — the attention scores — come from similarity between a query and a set of keys; this lets the model directly model relationships between any two positions and replace recurrence with parallel self-attention (with scaled dot-product attention and multi‑head attention to capture multiple types of relations) .  \n",
       "- Future of agents: the papers you provided argue that small, deployable language models (SLMs) — not giant generalist LLMs — are the likely future for most agentic systems because they are sufficiently capable for routine agent tasks, are more operationally suitable (latency, privacy, cost, local inference), and are economically preferable; the authors therefore advocate migrating many agent uses from large LLMs to SLMs and provide a conversion outline and case studies to support this view .\n",
       "\n",
       "More detail\n",
       "\n",
       "1) Core idea behind attention (Transformer perspective)\n",
       "- Attention computes interactions between elements by forming queries (Q), keys (K) and values (V). For each query you compute similarity scores to all keys, normalize (softmax) to get attention weights, and take the weighted sum of the corresponding values. This produces context-aware representations that directly capture dependencies regardless of distance in the sequence .  \n",
       "- Scaled dot‑product attention and multi‑head attention: scaling stabilizes gradients for large dimensionalities; multi‑head attention runs several attention computations in parallel (different linear projections) so the model can attend to different types of relationships simultaneously .  \n",
       "- Practical payoff: replacing recurrence/convolution with self‑attention yields much more parallelizable architectures and a constant (per‑layer) number of operations to relate any two positions, which improved training speed and quality in the Transformer experiments .\n",
       "\n",
       "2) What the provided paper says about the future of agents\n",
       "- Definitions & thesis: the authors define SLMs as language models small enough to run with low-latency on common consumer devices (rough rule: models < ~10B params as of 2025) and contrast them with LLMs; they state that SLMs are the future of agentic AI because they (1) are typically powerful enough for agent tasks, (2) are operationally better suited, and (3) are much more economical, so SLM adoption is a necessary outcome if practical priorities are followed .  \n",
       "- Rationale and implications: the paper argues current practice often overuses single, large generalist LLMs to handle agents’ requests; instead, many agent requests are simple and can be handled by smaller specialized models; moving to SLMs brings advantages in deployment cost, latency, privacy (on-device inference), and energy/compute efficiency; the authors propose an algorithmic pathway to migrate agentic applications from LLMs to SLMs and present case studies estimating the potential scope of replacement .  \n",
       "- Tone: this is not a claim that LLMs will disappear — rather that for the majority of practical agent workloads the tradeoffs favor smaller, local or specialized models as the mainstream solution going forward .\n",
       "\n",
       "If you’d like, I can:\n",
       "- Give a short diagram or math sketch of scaled dot‑product / multi‑head attention.  \n",
       "- Summarize the migration steps and case studies from the agent paper in more detail."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(response.output[-1].content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a0350a",
   "metadata": {},
   "source": [
    "### Continuing the Conversation\n",
    "\n",
    "One of the most powerful features of the Responses API is simple conversation continuity. Just pass the `previous_response_id` to maintain context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0b9586c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follow-up Query: Can you explain the methodology of each of these paper in 2 or 3 sentences?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Methodology of the Attention Mechanism in Transformers\n",
       "\n",
       "The Transformer model eliminates the reliance on recurrence by using self-attention mechanisms to compute representations of input and output sequences. The core methodology involves using an attention mechanism to model dependencies between words regardless of their distance in the sequence. This is achieved through scaled dot-product attention and multi-head attention, which allow multiple attention operations to be performed in parallel, providing the capability for the model to focus on different parts of the sequence at the same time.\n",
       "\n",
       "### Methodology of the Future of Agents Paper\n",
       "\n",
       "The paper presents a thesis that smaller language models (SLMs) are more suitable than large language models (LLMs) for agentic AI applications due to their operational and economic advantages. It outlines a methodology to transition from LLMs to SLMs, argues the case with operational and economic benefits, and defends this position through case studies. The paper proposes a conversion algorithm for migrating applications to use SLMs, supported by practical examples and cost analysis."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def continue_conversation(query: str, previous_response_id: str, vector_store_id: str, model: str = \"gpt-4o\"):\n",
    "    \"\"\"Continue a conversation using previous_response_id.\"\"\"\n",
    "    \n",
    "    instructions = \"\"\"You are a helpful research assistant. \n",
    "    Use the provided documentation to answer questions accurately.\n",
    "    Refer to our previous conversation when relevant.\"\"\"\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        input=query,\n",
    "        model=model,\n",
    "        instructions=instructions,\n",
    "        previous_response_id=previous_response_id,\n",
    "        tools=[{\n",
    "            \"type\": \"file_search\",\n",
    "            \"vector_store_ids\": [vector_store_id],\n",
    "            \"max_num_results\": 5\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Ask a follow-up question\n",
    "# vector_store_id = vector_store_details['id']\n",
    "follow_up_query = \"Can you explain the methodology of each of these paper in 2 or 3 sentences?\"\n",
    "print(f\"Follow-up Query: {follow_up_query}\\n\")\n",
    "\n",
    "follow_up_response = continue_conversation(\n",
    "    follow_up_query,\n",
    "    response.id,\n",
    "    vector_store_id\n",
    ")\n",
    "\n",
    "Markdown(follow_up_response.output[-1].content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8e3712",
   "metadata": {},
   "source": [
    "### Extracting Citations\n",
    "\n",
    "The Responses API provides annotations that include information about which files were used to generate the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bae529e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Files Referenced:\n",
      "  - future_agents.pdf\n",
      "  - attention_paper.pdf\n"
     ]
    }
   ],
   "source": [
    "def extract_citations(response):\n",
    "    \"\"\"Extract file citations from response annotations.\"\"\"\n",
    "    citations = set()\n",
    "    \n",
    "    for item in response.output:\n",
    "        if hasattr(item, 'content'):\n",
    "            for content in item.content:\n",
    "                if hasattr(content, 'annotations'):\n",
    "                    for annotation in content.annotations:\n",
    "                        if hasattr(annotation, 'filename'):\n",
    "                            citations.add(annotation.filename)\n",
    "    \n",
    "    return citations\n",
    "\n",
    "# Extract and display citations\n",
    "citations = extract_citations(follow_up_response)\n",
    "\n",
    "if citations:\n",
    "    print(\"\\nFiles Referenced:\")\n",
    "    for citation in citations:\n",
    "        print(f\"  - {citation}\")\n",
    "else:\n",
    "    print(\"\\nNo explicit file citations found in response.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75e0e49",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices and Cost Management\n",
    "\n",
    "### Cost Considerations\n",
    "\n",
    "- **File Search**: $2.50 per 1,000 queries + $0.10/GB/day storage (first GB free)\n",
    "- **Code Interpreter**: $0.03 per session\n",
    "- **Vector Storage**: Set expiration policies to manage costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store_with_expiration(name: str, days: int = 7):\n",
    "    \"\"\"Create a vector store with automatic expiration after inactivity.\"\"\"\n",
    "    \n",
    "    vector_store = client.vector_stores.create(\n",
    "        name=name,\n",
    "        expires_after={\n",
    "            \"anchor\": \"last_active_at\",\n",
    "            \"days\": days\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Created vector store: {name}\")\n",
    "    print(f\"Will expire after {days} days of inactivity\")\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "# Example: Create a temporary vector store\n",
    "temp_store = create_vector_store_with_expiration(\"Temporary Research Store\", days=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6ff4b",
   "metadata": {},
   "source": [
    "### Supported File Types\n",
    "\n",
    "File Search supports many text-based formats:\n",
    "- Documents: `.pdf`, `.docx`, `.txt`, `.md`\n",
    "- Code: `.py`, `.js`, `.java`, `.cpp`, `.html`, `.css`\n",
    "- Data: `.json`, `.csv`, `.xml`\n",
    "- Maximum file size: 512 MB\n",
    "- Maximum tokens per file: 5,000,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08739f47",
   "metadata": {},
   "source": [
    "### Cleanup Resources\n",
    "\n",
    "Remember to clean up resources when you're done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c30ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_resources(vector_store_ids=None, file_ids=None):\n",
    "    \"\"\"Clean up vector stores and files.\"\"\"\n",
    "    \n",
    "    if vector_store_ids:\n",
    "        for vs_id in vector_store_ids:\n",
    "            try:\n",
    "                client.vector_stores.delete(vs_id)\n",
    "                print(f\"Deleted vector store: {vs_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting vector store {vs_id}: {e}\")\n",
    "    \n",
    "    if file_ids:\n",
    "        for file_id in file_ids:\n",
    "            try:\n",
    "                client.files.delete(file_id)\n",
    "                print(f\"Deleted file: {file_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting file {file_id}: {e}\")\n",
    "\n",
    "# Example cleanup (uncomment to use)\n",
    "# cleanup_resources(\n",
    "#     vector_store_ids=[vector_store_details['id']],\n",
    "#     file_ids=[uploaded_file_id]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100cad61",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Migration Notes: Assistants API → Responses API\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Aspect | Assistants API (Old) | Responses API (New) |\n",
    "|--------|---------------------|--------------------|\n",
    "| **Conversation Management** | Thread-based, manual history | `previous_response_id` |\n",
    "| **Instructions** | Stored on server | Stateless, per-request |\n",
    "| **API Calls** | Create assistant → thread → message → run | Single `responses.create()` |\n",
    "| **Execution** | Asynchronous with polling | Synchronous or streaming |\n",
    "| **Complexity** | Higher (multiple objects) | Lower (unified) |\n",
    "\n",
    "### Code Comparison\n",
    "\n",
    "**Old Way (Assistants API):**\n",
    "```python\n",
    "# Create assistant\n",
    "assistant = client.beta.assistants.create(\n",
    "    instructions=\"You are helpful\",\n",
    "    tools=[{\"type\": \"file_search\"}]\n",
    ")\n",
    "\n",
    "# Create thread\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "# Add message\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"Hello\"\n",
    ")\n",
    "\n",
    "# Run assistant\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "# Get messages\n",
    "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "```\n",
    "\n",
    "**New Way (Responses API):**\n",
    "```python\n",
    "# Single call\n",
    "response = client.responses.create(\n",
    "    input=\"Hello\",\n",
    "    instructions=\"You are helpful\",\n",
    "    tools=[{\"type\": \"file_search\", \"vector_store_ids\": [vs_id]}]\n",
    ")\n",
    "\n",
    "# Continue conversation\n",
    "response2 = client.responses.create(\n",
    "    input=\"Follow up\",\n",
    "    previous_response_id=response.id,\n",
    "    tools=[{\"type\": \"file_search\", \"vector_store_ids\": [vs_id]}]\n",
    ")\n",
    "```\n",
    "\n",
    "### Benefits of Responses API\n",
    "\n",
    "1. **Simpler code**: Fewer API calls and objects to manage\n",
    "2. **Better performance**: Direct response without polling\n",
    "3. **Flexible state**: Choose stateful or stateless as needed\n",
    "4. **Easier debugging**: Unified request/response structure\n",
    "5. **Better typing**: Improved TypeScript definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4a6679",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Congratulations! You've learned how to use OpenAI's modern Responses API with both File Search and Code Interpreter tools.\n",
    "\n",
    "### What We Covered\n",
    "\n",
    "1. **File Search**: Build knowledge-based assistants that can search through documents\n",
    "2. **Code Interpreter**: Perform data analysis, create visualizations, and solve complex problems\n",
    "3. **Conversation Management**: Maintain context using `previous_response_id`\n",
    "4. **Combined Tools**: Use multiple tools together for powerful applications\n",
    "5. **Best Practices**: Cost management, file handling, and resource cleanup\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with your own documents and datasets\n",
    "- Combine tools creatively for your specific use cases\n",
    "- Explore the other tools available in the Responses API (web search, etc.)\n",
    "- Build production applications with proper error handling and monitoring\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [OpenAI Responses API Documentation](https://platform.openai.com/docs/api-reference/responses)\n",
    "- [File Search Guide](https://platform.openai.com/docs/guides/tools-file-search)\n",
    "- [OpenAI Cookbook Examples](https://cookbook.openai.com/)\n",
    "\n",
    "Happy building! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
