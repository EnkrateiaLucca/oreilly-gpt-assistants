{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5885d562",
   "metadata": {},
   "source": [
    "# Building AI Applications with OpenAI's Responses API\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to this comprehensive guide on using OpenAI's **Responses API**, the modern replacement for the deprecated Assistants API. In this tutorial, we'll explore how to build powerful AI applications that can search through documents and perform complex data analysis.\n",
    "\n",
    "### What's Changed?\n",
    "\n",
    "The Assistants API is being deprecated (sunset planned for mid-2026) in favor of the new **Responses API**, which offers:\n",
    "\n",
    "- **Simpler architecture**: No more complex thread management - just pass `previous_response_id` to continue conversations\n",
    "- **Unified experience**: Combines the best of Chat Completions and Assistants APIs\n",
    "- **Stateless design**: Instructions are stateless and apply only to the current request\n",
    "- **Better performance**: More efficient conversation management\n",
    "- **Same powerful tools**: File Search and Code Interpreter are both fully supported\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "By the end of this tutorial, you'll be able to:\n",
    "\n",
    "1. Use the **File Search** tool to build knowledge-based assistants\n",
    "2. Leverage the **Code Interpreter** tool for data analysis and visualization\n",
    "3. Manage vector stores for document retrieval\n",
    "4. Chain responses to maintain conversation context\n",
    "5. Combine multiple tools for powerful AI applications\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8325b056",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up our environment with the required imports and initialize our OpenAI client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc356ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai==2.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5acc07dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dae86af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "print(\"OpenAI client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c827566d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: File Search with the Responses API\n",
    "\n",
    "File Search allows your AI to access knowledge from documents you provide. OpenAI automatically handles:\n",
    "- Parsing and chunking documents\n",
    "- Creating and storing embeddings\n",
    "- Performing vector and keyword search\n",
    "- Retrieving relevant content to answer queries\n",
    "\n",
    "### Understanding Vector Stores\n",
    "\n",
    "**Vector Stores** are specialized databases for efficient storage and retrieval of information:\n",
    "- Files are automatically processed (parsed, chunked, embedded)\n",
    "- Support both keyword and semantic search\n",
    "- Can store up to 10,000 files each\n",
    "- Maximum file size: 512 MB per file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995e7ca7",
   "metadata": {},
   "source": [
    "### Creating a Vector Store\n",
    "\n",
    "Let's create a vector store for our documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d47ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(store_name: str):\n",
    "    \"\"\"Creates a vector store for document storage.\"\"\"\n",
    "    vector_store = client.vector_stores.create(name=store_name)\n",
    "    \n",
    "    details = {\n",
    "        \"id\": vector_store.id,\n",
    "        \"name\": vector_store.name,\n",
    "        \"created_at\": vector_store.created_at,\n",
    "        \"file_count\": vector_store.file_counts.completed\n",
    "    }\n",
    "    \n",
    "    print(f\"Created vector store: {vector_store.name}\")\n",
    "    print(f\"Vector Store ID: {vector_store.id}\")\n",
    "    \n",
    "    return details\n",
    "\n",
    "# Create our first vector store\n",
    "vector_store_details = create_vector_store(\"Product Documentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f3311b",
   "metadata": {},
   "source": [
    "### Uploading Files to the Vector Store\n",
    "\n",
    "Now let's upload documents to our vector store. We'll download a sample research paper from arXiv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6250be55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading research paper...\n",
      "Downloaded: ./pdfs/future_agents.pdf\n",
      "Downloading research paper...\n",
      "Downloaded: ./pdfs/attention_paper.pdf\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Download a sample research paper from arXiv\n",
    "\n",
    "def download_paper(paper_url: str, file_path: str):\n",
    "    \"\"\"Download a research paper from arXiv.\"\"\"\n",
    "    print(\"Downloading research paper...\")\n",
    "    response = requests.get(paper_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    print(f\"Downloaded: {file_path}\")\n",
    "    \n",
    "    return file_path\n",
    "\n",
    "\n",
    "\n",
    "file_paths = [\"./assets-resources/pdfs/future_agents.pdf\",\"./assets-resources/pdfs/attention_paper.pdf\"]\n",
    "\n",
    "paper_urls = [\n",
    "    \"https://arxiv.org/pdf/2506.02153\",\n",
    "    \"https://arxiv.org/pdf/1706.03762\"\n",
    "]\n",
    "\n",
    "for file_path, paper_url in zip(file_paths, paper_urls):\n",
    "    download_paper(paper_url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799f037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading future_agents.pdf...\n",
      "Adding to vector store...\n",
      "âœ“ Successfully uploaded: future_agents.pdf\n",
      "\n",
      "Waiting for file processing...\n",
      "Ready!\n",
      "Uploading attention_paper.pdf...\n",
      "Adding to vector store...\n",
      "âœ“ Successfully uploaded: attention_paper.pdf\n",
      "\n",
      "Waiting for file processing...\n",
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "def upload_file_to_vector_store(file_path: str, vector_store_id: str):\n",
    "    \"\"\"Uploads a file to the vector store.\"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    try:\n",
    "        # Upload file to OpenAI\n",
    "        print(f\"Uploading {file_name}...\")\n",
    "        file_response = client.files.create(\n",
    "            file=open(file_path, 'rb'),\n",
    "            purpose=\"assistants\"\n",
    "        )\n",
    "        \n",
    "        # Add file to vector store\n",
    "        print(f\"Adding to vector store...\")\n",
    "        client.vector_stores.files.create(\n",
    "            vector_store_id=vector_store_id,\n",
    "            file_id=file_response.id\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ“ Successfully uploaded: {file_name}\")\n",
    "        return {\"file\": file_name, \"status\": \"success\", \"file_id\": file_response.id}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Failed to upload {file_name}: {str(e)}\")\n",
    "        return {\"file\": file_name, \"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "# Upload our file\n",
    "# Created this vector store below manually in the openai platform\n",
    "# vector_store_id = \"vs_6900b46abc748191b31df0c8d561e5d7\"\n",
    "vector_store_id = vector_store_details['id']\n",
    "\n",
    "\n",
    "for file_path in file_paths:\n",
    "    upload_result = upload_file_to_vector_store(file_path, vector_store_id)\n",
    "    # Wait a moment for processing\n",
    "    print(\"\\nWaiting for file processing...\")\n",
    "    time.sleep(3)\n",
    "    print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafedaae",
   "metadata": {},
   "source": [
    "### Using File Search with the Responses API\n",
    "\n",
    "Now comes the exciting part! With the Responses API, we can query our documents directly without managing threads or assistants.\n",
    "\n",
    "The key difference from the old Assistants API:\n",
    "- **Old way**: Create assistant â†’ Create thread â†’ Add message â†’ Run assistant â†’ Poll for completion\n",
    "- **New way**: Simply call `responses.create()` with the file_search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146f0cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Explain the core idea behind the attention mechanism and what is the future of agents according to the papers you have access to.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def query_documents(query: str, vector_store_id: str, model: str = \"gpt-5-mini\"):\n",
    "    \"\"\"Query documents using file search in the Responses API.\"\"\"\n",
    "    \n",
    "    instructions = \"\"\"You are a helpful research assistant. \n",
    "    Use the provided documentation to answer questions accurately.\n",
    "    If you're not sure about something, admit it and stick to the information in the documents.\n",
    "    Always cite your sources when possible.\"\"\"\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        input=query,\n",
    "        model=model,\n",
    "        instructions=instructions,\n",
    "        tools=[{\n",
    "            \"type\": \"file_search\",\n",
    "            \"vector_store_ids\": [vector_store_id],\n",
    "            \"max_num_results\": 5\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "# Ask a question about our document\n",
    "query = \"Explain the core idea behind the attention mechanism and what is the future of agents according to the papers you have access to.\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "response = query_documents(query, vector_store_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a511350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Short answer first:\n",
       "\n",
       "- Attention: the core idea is to let the model compute, for each position (or query), a weighted sum of (value) representations where the weights â€” the attention scores â€” come from similarity between a query and a set of keys; this lets the model directly model relationships between any two positions and replace recurrence with parallel self-attention (with scaled dot-product attention and multiâ€‘head attention to capture multiple types of relations) .  \n",
       "- Future of agents: the papers you provided argue that small, deployable language models (SLMs) â€” not giant generalist LLMs â€” are the likely future for most agentic systems because they are sufficiently capable for routine agent tasks, are more operationally suitable (latency, privacy, cost, local inference), and are economically preferable; the authors therefore advocate migrating many agent uses from large LLMs to SLMs and provide a conversion outline and case studies to support this view .\n",
       "\n",
       "More detail\n",
       "\n",
       "1) Core idea behind attention (Transformer perspective)\n",
       "- Attention computes interactions between elements by forming queries (Q), keys (K) and values (V). For each query you compute similarity scores to all keys, normalize (softmax) to get attention weights, and take the weighted sum of the corresponding values. This produces context-aware representations that directly capture dependencies regardless of distance in the sequence .  \n",
       "- Scaled dotâ€‘product attention and multiâ€‘head attention: scaling stabilizes gradients for large dimensionalities; multiâ€‘head attention runs several attention computations in parallel (different linear projections) so the model can attend to different types of relationships simultaneously .  \n",
       "- Practical payoff: replacing recurrence/convolution with selfâ€‘attention yields much more parallelizable architectures and a constant (perâ€‘layer) number of operations to relate any two positions, which improved training speed and quality in the Transformer experiments .\n",
       "\n",
       "2) What the provided paper says about the future of agents\n",
       "- Definitions & thesis: the authors define SLMs as language models small enough to run with low-latency on common consumer devices (rough rule: models < ~10B params as of 2025) and contrast them with LLMs; they state that SLMs are the future of agentic AI because they (1) are typically powerful enough for agent tasks, (2) are operationally better suited, and (3) are much more economical, so SLM adoption is a necessary outcome if practical priorities are followed .  \n",
       "- Rationale and implications: the paper argues current practice often overuses single, large generalist LLMs to handle agentsâ€™ requests; instead, many agent requests are simple and can be handled by smaller specialized models; moving to SLMs brings advantages in deployment cost, latency, privacy (on-device inference), and energy/compute efficiency; the authors propose an algorithmic pathway to migrate agentic applications from LLMs to SLMs and present case studies estimating the potential scope of replacement .  \n",
       "- Tone: this is not a claim that LLMs will disappear â€” rather that for the majority of practical agent workloads the tradeoffs favor smaller, local or specialized models as the mainstream solution going forward .\n",
       "\n",
       "If youâ€™d like, I can:\n",
       "- Give a short diagram or math sketch of scaled dotâ€‘product / multiâ€‘head attention.  \n",
       "- Summarize the migration steps and case studies from the agent paper in more detail."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(response.output[-1].content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a0350a",
   "metadata": {},
   "source": [
    "### Continuing the Conversation\n",
    "\n",
    "One of the most powerful features of the Responses API is simple conversation continuity. Just pass the `previous_response_id` to maintain context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b9586c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follow-up Query: Can you explain the methodology of each of these paper in 2 or 3 sentences?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Methodology of the Attention Mechanism in Transformers\n",
       "\n",
       "The Transformer model eliminates the reliance on recurrence by using self-attention mechanisms to compute representations of input and output sequences. The core methodology involves using an attention mechanism to model dependencies between words regardless of their distance in the sequence. This is achieved through scaled dot-product attention and multi-head attention, which allow multiple attention operations to be performed in parallel, providing the capability for the model to focus on different parts of the sequence at the same time.\n",
       "\n",
       "### Methodology of the Future of Agents Paper\n",
       "\n",
       "The paper presents a thesis that smaller language models (SLMs) are more suitable than large language models (LLMs) for agentic AI applications due to their operational and economic advantages. It outlines a methodology to transition from LLMs to SLMs, argues the case with operational and economic benefits, and defends this position through case studies. The paper proposes a conversion algorithm for migrating applications to use SLMs, supported by practical examples and cost analysis."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def continue_conversation(query: str, previous_response_id: str, vector_store_id: str, model: str = \"gpt-4o\"):\n",
    "    \"\"\"Continue a conversation using previous_response_id.\"\"\"\n",
    "    \n",
    "    instructions = \"\"\"You are a helpful research assistant. \n",
    "    Use the provided documentation to answer questions accurately.\n",
    "    Refer to our previous conversation when relevant.\"\"\"\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        input=query,\n",
    "        model=model,\n",
    "        instructions=instructions,\n",
    "        previous_response_id=previous_response_id,\n",
    "        tools=[{\n",
    "            \"type\": \"file_search\",\n",
    "            \"vector_store_ids\": [vector_store_id],\n",
    "            \"max_num_results\": 5\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Ask a follow-up question\n",
    "follow_up_query = \"Can you explain the methodology of each of these paper in 2 or 3 sentences?\"\n",
    "print(f\"Follow-up Query: {follow_up_query}\\n\")\n",
    "\n",
    "follow_up_response = continue_conversation(\n",
    "    follow_up_query,\n",
    "    response.id,\n",
    "    vector_store_id\n",
    ")\n",
    "\n",
    "Markdown(follow_up_response.output[-1].content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8e3712",
   "metadata": {},
   "source": [
    "### Extracting Citations\n",
    "\n",
    "The Responses API provides annotations that include information about which files were used to generate the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bae529e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Files Referenced:\n",
      "  - future_agents.pdf\n",
      "  - attention_paper.pdf\n"
     ]
    }
   ],
   "source": [
    "def extract_citations(response):\n",
    "    \"\"\"Extract file citations from response annotations.\"\"\"\n",
    "    citations = set()\n",
    "    \n",
    "    for item in response.output:\n",
    "        if hasattr(item, 'content'):\n",
    "            for content in item.content:\n",
    "                if hasattr(content, 'annotations'):\n",
    "                    for annotation in content.annotations:\n",
    "                        if hasattr(annotation, 'filename'):\n",
    "                            citations.add(annotation.filename)\n",
    "    \n",
    "    return citations\n",
    "\n",
    "# Extract and display citations\n",
    "citations = extract_citations(follow_up_response)\n",
    "\n",
    "if citations:\n",
    "    print(\"\\nFiles Referenced:\")\n",
    "    for citation in citations:\n",
    "        print(f\"  - {citation}\")\n",
    "else:\n",
    "    print(\"\\nNo explicit file citations found in response.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75e0e49",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices and Cost Management\n",
    "\n",
    "### Cost Considerations\n",
    "\n",
    "- **File Search**: $2.50 per 1,000 queries + $0.10/GB/day storage (first GB free)\n",
    "- **Code Interpreter**: $0.03 per session\n",
    "- **Vector Storage**: Set expiration policies to manage costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store_with_expiration(name: str, days: int = 7):\n",
    "    \"\"\"Create a vector store with automatic expiration after inactivity.\"\"\"\n",
    "    \n",
    "    vector_store = client.vector_stores.create(\n",
    "        name=name,\n",
    "        expires_after={\n",
    "            \"anchor\": \"last_active_at\",\n",
    "            \"days\": days\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Created vector store: {name}\")\n",
    "    print(f\"Will expire after {days} days of inactivity\")\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "# Example: Create a temporary vector store\n",
    "temp_store = create_vector_store_with_expiration(\"Temporary Research Store\", days=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6ff4b",
   "metadata": {},
   "source": [
    "### Supported File Types\n",
    "\n",
    "File Search supports many text-based formats:\n",
    "- Documents: `.pdf`, `.docx`, `.txt`, `.md`\n",
    "- Code: `.py`, `.js`, `.java`, `.cpp`, `.html`, `.css`\n",
    "- Data: `.json`, `.csv`, `.xml`\n",
    "- Maximum file size: 512 MB\n",
    "- Maximum tokens per file: 5,000,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08739f47",
   "metadata": {},
   "source": [
    "### Cleanup Resources\n",
    "\n",
    "Remember to clean up resources when you're done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c30ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_resources(vector_store_ids=None, file_ids=None):\n",
    "    \"\"\"Clean up vector stores and files.\"\"\"\n",
    "    \n",
    "    if vector_store_ids:\n",
    "        for vs_id in vector_store_ids:\n",
    "            try:\n",
    "                client.vector_stores.delete(vs_id)\n",
    "                print(f\"Deleted vector store: {vs_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting vector store {vs_id}: {e}\")\n",
    "    \n",
    "    if file_ids:\n",
    "        for file_id in file_ids:\n",
    "            try:\n",
    "                client.files.delete(file_id)\n",
    "                print(f\"Deleted file: {file_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting file {file_id}: {e}\")\n",
    "\n",
    "# Example cleanup (uncomment to use)\n",
    "# cleanup_resources(\n",
    "#     vector_store_ids=[vector_store_details['id']],\n",
    "#     file_ids=[uploaded_file_id]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100cad61",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Migration Notes: Assistants API â†’ Responses API\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Aspect | Assistants API (Old) | Responses API (New) |\n",
    "|--------|---------------------|--------------------|\n",
    "| **Conversation Management** | Thread-based, manual history | `previous_response_id` |\n",
    "| **Instructions** | Stored on server | Stateless, per-request |\n",
    "| **API Calls** | Create assistant â†’ thread â†’ message â†’ run | Single `responses.create()` |\n",
    "| **Execution** | Asynchronous with polling | Synchronous or streaming |\n",
    "| **Complexity** | Higher (multiple objects) | Lower (unified) |\n",
    "\n",
    "### Code Comparison\n",
    "\n",
    "**Old Way (Assistants API):**\n",
    "```python\n",
    "# Create assistant\n",
    "assistant = client.beta.assistants.create(\n",
    "    instructions=\"You are helpful\",\n",
    "    tools=[{\"type\": \"file_search\"}]\n",
    ")\n",
    "\n",
    "# Create thread\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "# Add message\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"Hello\"\n",
    ")\n",
    "\n",
    "# Run assistant\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "# Get messages\n",
    "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "```\n",
    "\n",
    "**New Way (Responses API):**\n",
    "```python\n",
    "# Single call\n",
    "response = client.responses.create(\n",
    "    input=\"Hello\",\n",
    "    instructions=\"You are helpful\",\n",
    "    tools=[{\"type\": \"file_search\", \"vector_store_ids\": [vs_id]}]\n",
    ")\n",
    "\n",
    "# Continue conversation\n",
    "response2 = client.responses.create(\n",
    "    input=\"Follow up\",\n",
    "    previous_response_id=response.id,\n",
    "    tools=[{\"type\": \"file_search\", \"vector_store_ids\": [vs_id]}]\n",
    ")\n",
    "```\n",
    "\n",
    "### Benefits of Responses API\n",
    "\n",
    "1. **Simpler code**: Fewer API calls and objects to manage\n",
    "2. **Better performance**: Direct response without polling\n",
    "3. **Flexible state**: Choose stateful or stateless as needed\n",
    "4. **Easier debugging**: Unified request/response structure\n",
    "5. **Better typing**: Improved TypeScript definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4a6679",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Congratulations! You've learned how to use OpenAI's modern Responses API with both File Search and Code Interpreter tools.\n",
    "\n",
    "### What We Covered\n",
    "\n",
    "1. **File Search**: Build knowledge-based assistants that can search through documents\n",
    "2. **Code Interpreter**: Perform data analysis, create visualizations, and solve complex problems\n",
    "3. **Conversation Management**: Maintain context using `previous_response_id`\n",
    "4. **Combined Tools**: Use multiple tools together for powerful applications\n",
    "5. **Best Practices**: Cost management, file handling, and resource cleanup\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with your own documents and datasets\n",
    "- Combine tools creatively for your specific use cases\n",
    "- Explore the other tools available in the Responses API (web search, etc.)\n",
    "- Build production applications with proper error handling and monitoring\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [OpenAI Responses API Documentation](https://platform.openai.com/docs/api-reference/responses)\n",
    "- [File Search Guide](https://platform.openai.com/docs/guides/tools-file-search)\n",
    "- [OpenAI Cookbook Examples](https://cookbook.openai.com/)\n",
    "\n",
    "Happy building! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
