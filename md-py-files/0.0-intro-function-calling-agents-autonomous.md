# Can you SEE MY SCREEN?

<span style="color: red;"> 
How do we connect LLMs with Tools?
</span>

## Introduction

This notebook demonstrates how to connect LLMs with Tools using function calling, from scratch. We'll build a simple example showing how to make Python functions available to an LLM and execute them autonomously.

## Setup

```python
# if you don't have an API key as an environment variable you can set it here
import os
import getpass

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"var: ")

_set_env("OPENAI_API_KEY")
```

```python
from openai import OpenAI
import os

client = OpenAI()
```

## LLM Response Example

```python
def llm_response(prompt: str):
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
    )
    return response.choices[0].message.content

llm_response("How do we connect LLMs with Tools? Output should be a single sentence.")
```

Output: `'LLMs can be connected with tools by using API integrations that allow the model to send queries and receive responses, enabling dynamic interactions with external services or databases.'`

## Two Approaches to Tool Connection

We could ask the model to:

1. Write Python code and then give the model the ability to run that code somewhere
2. Create the Python function that executes the task, then somehow give the model the ability to run that function autonomously

**Option number 2 is what we call FUNCTION CALLING!**

## Function Calling from Scratch

1. Write a Python function
2. Make that function "available" to the LLM model of choice (in this case will be gpt-4o/gpt-4o-mini)
3. Test if the model can execute that function properly
   1. Prepare the payload for the function (the arguments for it...)
   2. Write the function call
   3. We run the function call ourselves
   4. We inspect the results

## Example Functions

```python
def create_folder(folder_path: str):
    os.makedirs(folder_path, exist_ok=True)
    return f"Folder created at {folder_path}"

create_folder("./pancakes-are-the-best")
```

Output: `'Folder created at ./pancakes-are-the-best'`

```python
def write_file(file_path: str, content: str):
    with open(file_path, "w") as file:
        file.write(content)
    return f"File created at {file_path}"

write_file("./pancakes-are-the-best/pancakes-are-the-best.txt", "Pancakes are the best!")
```

Output: `'File created at ./pancakes-are-the-best/pancakes-are-the-best.txt'`

## LLM + Python Functions

### The Most Silly Way First

```python
prompt_with_function_information = """
You are a personal assistant with desktop capabilities. 
Users will ask you to perform tasks and you will execute those tasks by calling one or more of the following functions:

- create_folder(folder_path: str)
- write_file(file_path: str, content: str)

For example if the user asks: 

'Create a folder called lucas-teaches-function-callling'

Your output should be code like this:
'create_folder("./lucas-teaches-function-callling")'

'Create a folder called pancakes-are-the-best and write a file called pancakes-are-the-best.txt with the content "Pancakes are the best!"'

Your output should be code like this:

['create_folder("./pancakes-are-the-best")', 'write_file("./pancakes-are-the-best/pancakes-are-the-best.txt", "Pancakes are the best!")']

Your OUTPUT should ALWAYS BE python code only.

Here is the user prompt:
"""

user_prompt = "Create a file named pancakes-are-better-than-waffles.md with a one paragraph summary for why pancakes are better, \
make sure to address EG and BS students from my course who dared to say waffles are better."
```

```python
prompt = prompt_with_function_information + user_prompt
output_with_python_code_and_ticks = llm_response(prompt)
output_with_python_code_and_ticks
```

Output: `'```python\nwrite_file("./pancakes-are-better-than-waffles.md", "Pancakes are better than waffles for several reasons. Firstly, pancakes have a fluffy texture that can be perfectly customized with ingredients like blueberries or chocolate chips, providing a delightful burst of flavor in every bite. In contrast, waffles, while crispy, often lack the same versatility in taste. Furthermore, for my dear EG and BS students who dared to claim that waffles reign supreme, it\'s essential to recognize that pancakes can be made into stacks, drenched in syrup, or layered with fruits, creating a more satisfying breakfast experience. Ultimately, pancakes offer infinite possibilities for creativity and enjoyment, making them the superior choice.")\n```'`

### Clean Up the Output

```python
output_cleaned = output_with_python_code_and_ticks.replace("```python\n", "").replace("```", "")
output_cleaned
```

Output: `'write_file("./pancakes-are-better-than-waffles.md", "Pancakes are better than waffles for several reasons. Firstly, pancakes have a fluffy texture that can be perfectly customized with ingredients like blueberries or chocolate chips, providing a delightful burst of flavor in every bite. In contrast, waffles, while crispy, often lack the same versatility in taste. Furthermore, for my dear EG and BS students who dared to claim that waffles reign supreme, it\'s essential to recognize that pancakes can be made into stacks, drenched in syrup, or layered with fruits, creating a more satisfying breakfast experience. Ultimately, pancakes offer infinite possibilities for creativity and enjoyment, making them the superior choice.")\n'`

### Execute the Code

```python
exec(output_cleaned)
```

## Improved Function Call Parser

```python
import re

def parse_llm_function_calls(llm_output: str):
    """
    Naive parser that looks for function calls in the model output (e.g. 'create_folder("test")')
    and returns a list of dicts, each containing 'function_name' and 'args'.
    """
    calls = []
    
    # Regex looks for function_name( ... ) 
    # capturing everything until the next matching parenthesis
    pattern = r"(\w+)\(([^)]*)\)"
    matches = re.findall(pattern, llm_output)
    
    for match in matches:
        function_name = match[0]
        raw_args = match[1].split(',')
        
        # Clean up quotes/spaces
        parsed_args = [arg.strip().strip('"').strip("'") for arg in raw_args]
        calls.append({
            "function_name": function_name,
            "args": parsed_args
        })
    
    return calls

def execute_llm_function_calls(calls):
    """
    Executes each call by mapping the function_name to an actual Python function.
    If a function is unknown, handle gracefully.
    """
    for call in calls:
        fn_name = call["function_name"]
        args = call["args"]
        
        if fn_name == "create_folder":
            create_folder(*args)
        elif fn_name == "write_file":
            write_file(*args)
        else:
            print(f"[Warning] Unknown function: {fn_name}")

# Example usage
model_generated_code = """
create_folder("./some_folder")
write_file("./some_folder/readme.txt", "Some content")
"""

# 1) Parse
calls = parse_llm_function_calls(model_generated_code)
print(calls)

# 2) Execute
execute_llm_function_calls(calls)
```

Output: `[{'function_name': 'create_folder', 'args': ['./some_folder']}, {'function_name': 'write_file', 'args': ['./some_folder/readme.txt', 'Some content']}]`

## Conclusion

We connected LLM + FUNCTION via some simple PROMPT magic! This demonstrates the basic principle of function calling where:

1. We define functions that can be called
2. We instruct the LLM on how to use these functions
3. We parse the LLM output to extract function calls
4. We execute the functions and return results

This forms the foundation for more advanced AI agent systems that can interact with external tools and APIs.